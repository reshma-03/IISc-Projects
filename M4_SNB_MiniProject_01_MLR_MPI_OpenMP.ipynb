{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reshma-03/IISc-Projects/blob/main/M4_SNB_MiniProject_01_MLR_MPI_OpenMP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "heated-queens",
      "metadata": {
        "id": "heated-queens"
      },
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A program by IISc and TalentSprint\n",
        "### Mini-Project: Implementation of Multiple Linear Regression using MPI, OpenMP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "n-kxaHhwXEp9",
      "metadata": {
        "id": "n-kxaHhwXEp9"
      },
      "source": [
        "**DISCLAIMER:** THIS NOTEBOOK IS PROVIDED ONLY AS A REFERENCE SOLUTION NOTEBOOK FOR THE MINI-PROJECT. THERE MAY BE OTHER POSSIBLE APPROACHES/METHODS TO ACHIEVE THE SAME RESULTS."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "military-proportion",
      "metadata": {
        "id": "military-proportion"
      },
      "source": [
        "## Learning Objectives"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "durable-grounds",
      "metadata": {
        "id": "durable-grounds"
      },
      "source": [
        "At the end of the mini-project, you will be able to :\n",
        "\n",
        "* understand the collective communication operations like scatter, gather, broadcast\n",
        "* understand the blocking and non-blocking communication\n",
        "* implement multiple linear regression and run it using MPI\n",
        "* implement the multiple linear regression based predictions using OpenMP"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "growing-queens",
      "metadata": {
        "id": "growing-queens"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "raised-connection",
      "metadata": {
        "id": "raised-connection"
      },
      "source": [
        "The dataset chosen for this mini-project is [Combined Cycle Power Plant](https://archive.ics.uci.edu/ml/datasets/combined+cycle+power+plant). The dataset is made up of 9568 records and 5 columns. Each record contains the values for Ambient Temperature, Exhaust Vaccum, Ambient Pressure, Relative Humidity and Energy Output.\n",
        "\n",
        "Predicting full load electrical power output of a base load power plant is important in order to maximize the profit from the available megawatt hours.  The base load operation of a power plant is influenced by four main parameters, which are used as input variables in the dataset, such as ambient temperature, atmospheric pressure, relative humidity, and exhaust steam pressure. These parameters affect electrical power output, which is considered as the target variable.\n",
        "\n",
        "**Note:** The data was collected over a six year period (2006-11)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dominant-residence",
      "metadata": {
        "id": "dominant-residence"
      },
      "source": [
        "## Information"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "coated-timing",
      "metadata": {
        "id": "coated-timing"
      },
      "source": [
        "#### MPI in a Nutshell\n",
        "\n",
        "MPI stands for \"Message Passing Interface\". It is a library of functions (in C / Python) or subroutines (in Fortran) that you insert into source code to perform data communication between processes. MPI was developed over two years of discussions led by the MPI Forum, a group of roughly sixty people representing some forty organizations.\n",
        "\n",
        "To know more about MPI click [here](https://hpc-tutorials.llnl.gov/mpi/)\n",
        "\n",
        "\n",
        "#### Multiple Linear Regression\n",
        "\n",
        "Multiple regression is an extension of simple linear regression. It is used when we want to predict the value of a variable based on the value of two or more other variables. The variable we want to predict is called the dependent variable (or sometimes, the outcome, target or criterion variable). The variables we are using to predict the value of the dependent variable are called the independent variables (or sometimes, the predictor, explanatory or regressor variables)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "global-savings",
      "metadata": {
        "id": "global-savings"
      },
      "source": [
        "**Note:** We will be using the mpi4py Python package for MPI based code implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ndQNKsjS7c04",
      "metadata": {
        "id": "ndQNKsjS7c04"
      },
      "source": [
        "## Grading = 20 Points"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "green-deviation",
      "metadata": {
        "id": "green-deviation"
      },
      "source": [
        "**Run the below code to install mpi4py package**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "designing-marketing",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "designing-marketing",
        "scrolled": true,
        "outputId": "c5890c85-316b-4950-9a54-cda370ce9247"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mpi4py\n",
            "  Downloading mpi4py-4.0.0.tar.gz (464 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m464.8/464.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: mpi4py\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-4.0.0-cp310-cp310-linux_x86_64.whl size=4266273 sha256=6eb5c002f808605bf577f104f459caafd6bcc914ccb862027fcb2aa9d04b5409\n",
            "  Stored in directory: /root/.cache/pip/wheels/96/17/12/83db63ee0ae5c4b040ee87f2e5c813aea4728b55ec6a37317c\n",
            "Successfully built mpi4py\n",
            "Installing collected packages: mpi4py\n",
            "Successfully installed mpi4py-4.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install mpi4py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dedicated-thong",
      "metadata": {
        "id": "dedicated-thong"
      },
      "source": [
        "#### Importing Necessary Packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "reported-acrobat",
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "reported-acrobat",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Importing pandas\n",
        "import pandas as pd\n",
        "# Importing Numpy\n",
        "import numpy as np\n",
        "# Importing MPI from mpi4py package\n",
        "from mpi4py import MPI\n",
        "# Importing sqrt function from the Math\n",
        "from math import sqrt\n",
        "# Importing Decimal, ROUND_HALF_UP functions from the decimal package\n",
        "from decimal import Decimal, ROUND_HALF_UP\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "universal-jonathan",
      "metadata": {
        "cellView": "form",
        "id": "universal-jonathan",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "#@title Downloading the data\n",
        "!wget -qq https://cdn.iisc.talentsprint.com/CDS/Datasets/PowerPlantData.csv"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "forty-still",
      "metadata": {
        "id": "forty-still"
      },
      "source": [
        "### Overview\n",
        "\n",
        "* Load the data and perform data pre-processing\n",
        "* Identify the features, target and split the data into train and test\n",
        "* Implement multiple Linear Regression by estimating the coefficients on the given data\n",
        "* Use MPI package to distribute the data and implement `communicator`\n",
        "* Define functions for each objective and make a script (.py) file to execute using MPI command\n",
        "* Use OpenMP component to predict the data and calculate the error on the predicted data\n",
        "* Implement the Linear Regression from `sklearn` and compare the results"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "early-peace",
      "metadata": {
        "id": "early-peace"
      },
      "source": [
        "#### Exercise 1: Load data (1 point)\n",
        "\n",
        "Write a function that takes the filename as input and loads the data in a pandas dataframe with the column names as Ambient Temperature, Exhaust Vaccum, Ambient Pressure, Relative Humidity and Energy Output respectively.\n",
        "\n",
        "**Hint:** read_csv()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "differential-vacation",
      "metadata": {
        "id": "differential-vacation",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "FILENAME = \"/content/PowerPlantData.csv\" # Storing File path\n",
        "# Defining a function to load the data\n",
        "def loadData(filename):\n",
        "    # Loading the dataset with column names as\n",
        "    data = pd.read_csv(filename, header=0 , names = ['AmbientTemperature', 'ExhaustVaccum', 'AmbientPressure', 'RelativeHumidity', 'EnergyOutput'])\n",
        "    # Returning the dataframe\n",
        "    return data\n",
        "# Calling the function loadData and storing the dataframe in a variable named df\n",
        "df = loadData(FILENAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "italian-expense",
      "metadata": {
        "id": "italian-expense"
      },
      "source": [
        "#### Exercise 2: Explore data (1 point)\n",
        "\n",
        "Write a function that takes the data loaded using the above defined function as input and explore it.\n",
        "\n",
        "**Hint:** You can define and check for following things in the dataset inside a function\n",
        "\n",
        "- checking for the number of rows and columns\n",
        "- summary of the dataset\n",
        "- check for the null values\n",
        "- check for the duplicate values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "local-quarter",
      "metadata": {
        "id": "local-quarter",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Defining a function\n",
        "def exploreData(data):\n",
        "    print(data.shape) # Checking for number of rows and columns\n",
        "    print(data.describe()) # Summary of the data\n",
        "    print(data.isna()) # Checking for the null values in the data\n",
        "    print(sum(data.duplicated()))  # Checking for the duplicate values in the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rapid-mobile",
      "metadata": {
        "id": "rapid-mobile",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Calling the function exploreData to understand the dataset\n",
        "exploreData(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "whole-retailer",
      "metadata": {
        "id": "whole-retailer"
      },
      "source": [
        "#### Exercise 3: Handle missing data (1 point)\n",
        "\n",
        "After exploring the dataset if there are any null values present in the dataset then define a function that takes data loaded using the above defined function as input and handle the null values accordingly.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "- Drop the records containing the null values - dropna()\n",
        "- Replace the null values with the mean/median/mode - fillna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "incorporated-child",
      "metadata": {
        "id": "incorporated-child",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Function to handle missing data\n",
        "def handleMissingData(data):\n",
        "    data = data.dropna() # dropping the records containing null values using dropna function\n",
        "    # returning the dataframe after dropping the values\n",
        "    return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "intense-statistics",
      "metadata": {
        "id": "intense-statistics",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "newdf = handleMissingData(df) # storing the data after removing the null values from it"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "loaded-arbitration",
      "metadata": {
        "id": "loaded-arbitration"
      },
      "source": [
        "#### Exercise 4: Scale the data (1 point)\n",
        "\n",
        "Write a function that takes the data after handling the missing data as input and returns the standardized data.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "- standardization of the data  can be performed using the below formula\n",
        "\n",
        "$ (x - mean(x)) / std(x) $"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "extraordinary-qatar",
      "metadata": {
        "id": "extraordinary-qatar",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Defining a function to standardize the data\n",
        "def standardizeData(dataFile):\n",
        "    # Applying standardization formula\n",
        "    dataFile = (dataFile - dataFile.mean()) / dataFile.std()\n",
        "    # returning the standardization data\n",
        "    return dataFile"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ranking-viking",
      "metadata": {
        "id": "ranking-viking",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "ScaledData = standardizeData(newdf) # Storing the data after applying standardization on the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thermal-rehabilitation",
      "metadata": {
        "id": "thermal-rehabilitation"
      },
      "source": [
        "#### Exercise 5: Feature selection (1 point)\n",
        "\n",
        "Write a function that takes scaled data as input and returns the features and target variable values\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "- Features: AmbientTemperature, ExhaustVaccum, AmbientPressure, RelativeHumidity\n",
        "- Target Variable: EnergyOutput"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "terminal-starter",
      "metadata": {
        "id": "terminal-starter",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Function which returns features and target variables\n",
        "def FeatureSelector(data, target_name):\n",
        "    target = data[target_name] # Storing the target values\n",
        "    features = data.drop([target_name],axis=1) # Storing the features by dropping the target variable column\n",
        "    return features, target # Returning the features and target"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hungarian-repeat",
      "metadata": {
        "id": "hungarian-repeat",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "features, target = FeatureSelector(ScaledData,'EnergyOutput') # Storing the features and targets in variables respectively"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "timely-bruce",
      "metadata": {
        "id": "timely-bruce"
      },
      "source": [
        "#### Exercise 6: Correlation (1 point)\n",
        "\n",
        "Calculate correlation between the variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "durable-making",
      "metadata": {
        "id": "durable-making",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def correlation_factor(features):\n",
        "    corr = features.corr()\n",
        "    print(np.triu(corr))\n",
        "    return np.triu(corr)\n",
        "\n",
        "import seaborn as sns\n",
        "sns.heatmap(correlation_factor(features),annot=True,xticklabels=features.columns, yticklabels=features.columns)\n",
        "# heatmap is optional"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "honest-remainder",
      "metadata": {
        "id": "honest-remainder"
      },
      "source": [
        "#### Exercise 7: Estimate the coefficients (2 points)\n",
        "\n",
        "Write a function that takes features and target as input and returns the estimated coefficient values\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "- Calculate the estimated coefficients using the below formula\n",
        "\n",
        "$ β = (X^T X)^{-1} X^T y $\n",
        "\n",
        "- transpose(), np.linalg.inv()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dimensional-victory",
      "metadata": {
        "id": "dimensional-victory",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Calculating the coefficients\n",
        "def estimatedCoefficients(x, y):\n",
        "    # Implementing above formula\n",
        "    xT = x.transpose() # Transpose of x\n",
        "    inversed = np.linalg.inv( xT.dot(x) ) # Inverse of a matrix\n",
        "    coefficients = inversed.dot( xT ).dot(y) # performing final dot operation\n",
        "    # Returning the coefficients\n",
        "    return coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "activated-packaging",
      "metadata": {
        "id": "activated-packaging",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "estimatedCoefficients(features, target) # Calculating the estimatedCoefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "interior-bennett",
      "metadata": {
        "id": "interior-bennett"
      },
      "source": [
        "#### Exercise 8: Fit the data to estimate the coefficients (2 points)\n",
        "\n",
        "Write a function named fit which takes features and targets as input and returns the intercept and coefficient values.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "- create a dummy column in the features dataframe which is made up of all ones\n",
        "- convert the features dataframe into numpy array\n",
        "- call the estimated coefficients function which is defined above\n",
        "- np.ones(), np.concatenate()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "respected-holiday",
      "metadata": {
        "id": "respected-holiday",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# function to add dummy column into features dataframe and converting it into numpy array\n",
        "def dummyvariable(features):\n",
        "    # create a array of ones\n",
        "    m = np.ones((features.shape[0],1))\n",
        "    # combining the array of ones with features array\n",
        "    f = np.concatenate((m,features),axis=1)\n",
        "    # returning the features array\n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "local-texas",
      "metadata": {
        "id": "local-texas",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# defining a fit function\n",
        "def fit(x, y):\n",
        "    # prepare x and y values for coefficient estimates\n",
        "    x = dummyvariable(x) # adding a dummy column\n",
        "    print(x)\n",
        "    #y = y.values\n",
        "    betas = estimatedCoefficients(x, y) # calculating the estimated coefficients\n",
        "    # intercept becomes a vector of ones\n",
        "    intercept = betas[0]\n",
        "    # coefficients becomes the rest of the betas\n",
        "    coefficients = betas[1:]\n",
        "    # returning the intercept and coefficients\n",
        "    return intercept, coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "informational-vessel",
      "metadata": {
        "id": "informational-vessel",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "intercept, coefficients = fit(features, target) # fitting the data and calculating the intercept and coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hybrid-quick",
      "metadata": {
        "id": "hybrid-quick"
      },
      "source": [
        "#### Exercise 9: Predict the data on estimated coefficients (1 point)\n",
        "\n",
        "Write a function named predict which takes features, intercept and coefficient values as input and returns the predicted values.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "- Fit the intercept, coefficients values in the below equation\n",
        "\n",
        "  $y = b_0 + b_1*x + ... + b_i*x_i$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "buried-attention",
      "metadata": {
        "id": "buried-attention",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# fucntion to predict the values\n",
        "def predict(x, intercept, coefficients):\n",
        "    '''\n",
        "    y = b_0 + b_1*x + ... + b_i*x_i\n",
        "    '''\n",
        "    predictions = [] # Defining empty list to store the predicted values\n",
        "    for index, row in x.iterrows(): # iterating over features\n",
        "        values = row.values # converting each row into a array\n",
        "        pred = np.multiply(values, coefficients) # multiply the coefficients with the features values\n",
        "        pred = sum(pred) # storing the sum of each features\n",
        "        pred += intercept # finally adding the intercept value\n",
        "        predictions.append(pred) # appending the values to the list\n",
        "    # returning the predictions\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "falling-project",
      "metadata": {
        "id": "falling-project",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "predict(features, intercept, coefficients ) # Calling the predict function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rolled-consultancy",
      "metadata": {
        "id": "rolled-consultancy"
      },
      "source": [
        "#### Exercise 10: Root mean squared error (1 point)\n",
        "\n",
        "Write a function to calculate the RMSE error.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "- [How to calculate the RSME error](https://towardsdatascience.com/what-does-rmse-really-mean-806b65f2e48e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "phantom-alabama",
      "metadata": {
        "id": "phantom-alabama",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# function to calculate the error\n",
        "def rmse(actual, predicted):\n",
        "        # To store the value\n",
        "        sum_err = 0.0\n",
        "        # iterating over the actual values\n",
        "        for i in range(len(actual)):\n",
        "            # calculating mean squared error\n",
        "            pred_err = predicted[i] - actual[i]\n",
        "            sum_err += pred_err ** 2\n",
        "        mean_err = sum_err / float(len(actual))\n",
        "        # squaring the mean squared error to get the RMSE error value\n",
        "        return sqrt(mean_err)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "experimental-discrimination",
      "metadata": {
        "id": "experimental-discrimination"
      },
      "source": [
        "#### Exercise 11: Split the data into train and test (1 point)\n",
        "\n",
        "Write a function named train_test_split which takes features and targets as input and returns the train and test sets respectively.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "- Shuffle the data\n",
        "- Consider 70 % of data as a train set and the rest of the data as a test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dangerous-salmon",
      "metadata": {
        "id": "dangerous-salmon",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Divide the data into 70% train set and 30% test set\n",
        "def train_test_split(features_X , expected_target_Y ):\n",
        "    #Randomly pick 70% 0f the data\n",
        "    set_of_data = np.random.rand(len(features_X)) <= 0.7\n",
        "    X_train = features_X[set_of_data] # Training features set\n",
        "    Y_train = expected_target_Y[set_of_data] # target train set\n",
        "    #the remaining 30% is for the test set\n",
        "    X_test  = features_X[~set_of_data] # Test features set\n",
        "    Y_test  = expected_target_Y[~set_of_data] # target test sets\n",
        "    # Returning the train features set, train targets set, test features sets, test target sets\n",
        "    return X_train, X_test, Y_train, Y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lLR4ELZOIehN",
      "metadata": {
        "id": "lLR4ELZOIehN"
      },
      "source": [
        "#### Exercise 12: Implement predict using OpenMP (1 point)\n",
        "\n",
        "Get the predictions for test data and calculate the test error(RMSE) by implementing the OpenMP (pymp)\n",
        "\n",
        "**Hints:**\n",
        "\n",
        "* Using the pymp.Parallel implement the predict function (use from above)\n",
        "\n",
        "* Call the predict function by passing test data as an argument\n",
        "\n",
        "* calculate the error (RMSE) by comparing the Actual test data and predicted test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X_65P0TxIyYM",
      "metadata": {
        "id": "X_65P0TxIyYM"
      },
      "outputs": [],
      "source": [
        "!pip install pymp-pypi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tYh3pq9oI70J",
      "metadata": {
        "id": "tYh3pq9oI70J"
      },
      "outputs": [],
      "source": [
        "import pymp\n",
        "def predict(x, intercept, coefficients):\n",
        "    '''\n",
        "    y = b_0 + b_1*x + ... + b_i*x_i\n",
        "    '''\n",
        "    st = time.perf_counter()\n",
        "    predictions = pymp.shared.array(Y_test.shape) # Defining empty list to store the predicted values\n",
        "    with pymp.Parallel(4) as p:\n",
        "      for index in p.range(len(x)): # iterating over features\n",
        "          values = x[index] # converting eaach row into a array\n",
        "          pred = np.multiply(values, coefficients) # multiply the coefficients with the features values\n",
        "          pred = sum(pred) # storing the sum of each features\n",
        "          pred += intercept # finally adding the intercept value\n",
        "          predictions[index]= pred # appending the values to the list\n",
        "    # returning the predictions\n",
        "    print(time.perf_counter() - st)\n",
        "\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HM1azKJFLV7_",
      "metadata": {
        "id": "HM1azKJFLV7_"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, Y_train, Y_test =  train_test_split(features, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OpNpVHFQLMgC",
      "metadata": {
        "id": "OpNpVHFQLMgC"
      },
      "outputs": [],
      "source": [
        "# fit the data with x, y to calculate the coefficients\n",
        "b0_openmp, new_coefficients_openmp = fit(X_train, Y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xQeibuzrJCKb",
      "metadata": {
        "id": "xQeibuzrJCKb"
      },
      "outputs": [],
      "source": [
        "# predicting the test data\n",
        "test_predictions = np.array(predict(X_test.values, b0_openmp, new_coefficients_openmp))\n",
        "\n",
        "# calculating the error\n",
        "print(\"Test set error(RMSE) is {}\" .format(rmse(Y_test.values, test_predictions)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "covered-canon",
      "metadata": {
        "id": "covered-canon"
      },
      "source": [
        "#### Exercise 13: Create a communicator (1 point)\n",
        "\n",
        "Create a comunicator and define the rank and size"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "radio-origin",
      "metadata": {
        "id": "radio-origin",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Initialize communicator\n",
        "comm = MPI.COMM_WORLD\n",
        "# ID of the current worker\n",
        "rank = comm.Get_rank()\n",
        "# Rank ID of sender\n",
        "status = MPI.Status()\n",
        "# Number of workers\n",
        "size = comm.Get_size()\n",
        "root = 0   # Root\n",
        "# to calculate time\n",
        "t_start = 0\n",
        "t_diff = 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "miniature-plaza",
      "metadata": {
        "id": "miniature-plaza"
      },
      "source": [
        "#### Exercise 14: Divide the data into slices (1 point)\n",
        "\n",
        "Write a function named dividing_data which takes train features set, train target set, and size of workers as inputs and returns the sliced data for each worker.\n",
        "\n",
        "![img](https://cdn.iisc.talentsprint.com/CDS/Images/MiniProject_MPI_DataSlice.JPG)\n",
        "\n",
        "For Example, if there are 4 processes, slice the data into 4 equal parts with 25% ratio\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "- Divide the Data equally among the workers\n",
        "  - Create an empty list\n",
        "  - Iterate over the size of workers\n",
        "  - Append each slice of data to the list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "signal-medicaid",
      "metadata": {
        "id": "signal-medicaid",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "def dividing_data(x_train, y_train, size_of_workers):\n",
        "    #Divide the data among the workers\n",
        "    slice_for_each_worker = int(Decimal(x_train.shape[0]/size_of_workers).quantize(Decimal('1.'), rounding = ROUND_HALF_UP))\n",
        "    print('Slice of data for each worker: {}'.format(slice_for_each_worker))\n",
        "    x_data_for_worker = []\n",
        "    y_data_for_worker = []\n",
        "    for i in range(0,size_of_workers):\n",
        "        if i < size_of_workers - 1:\n",
        "            x_data_for_worker.append(x_train[slice_for_each_worker*i:slice_for_each_worker*(i+1)])\n",
        "            y_data_for_worker.append(y_train[slice_for_each_worker*i:slice_for_each_worker*(i+1)])\n",
        "        else:\n",
        "            x_data_for_worker.append(x_train[slice_for_each_worker*i:])\n",
        "            y_data_for_worker.append(y_train[slice_for_each_worker*i:])\n",
        "    return x_data_for_worker, y_data_for_worker\n",
        "\n",
        "# Alternate way is to use np.split()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "changing-conditioning",
      "metadata": {
        "id": "changing-conditioning"
      },
      "source": [
        "#### Exercise 15: Prepare the data in root worker to assign data for all the workers (1 point)\n",
        "\n",
        "- When it is the root worker, perform the below operation:\n",
        "    - Store the features and target values in separate variables\n",
        "    - Split the data into train and test sets using the train_test_split function defined above\n",
        "    - Divide the data among the workers using the dividing_data function above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hybrid-tamil",
      "metadata": {
        "id": "hybrid-tamil",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "if rank == root:\n",
        "    t_start = MPI.Wtime()\n",
        "    # Splitting the data into train and test\n",
        "    X_train, X_test, Y_train, Y_test =  train_test_split(features, target)\n",
        "    #Divide the data among the workers\n",
        "    x_data_for_worker, y_data_for_worker = dividing_data(X_train, Y_train, size)\n",
        "wt = MPI.Wtime()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "breathing-baking",
      "metadata": {
        "id": "breathing-baking"
      },
      "source": [
        "#### Exercise 16: Scatter and gather the data (1 point)\n",
        "\n",
        "Perform the below operations:\n",
        "\n",
        "- Send slices of the training set(the features data X and the expected target data Y) to every worker including the root worker\n",
        "    - **Hint:** scatter()\n",
        "    - use `barrier()` to block workers until all workers in the group reach a Barrier, to scatter from root worker.\n",
        "- Every worker should get the predicted target Y(yhat) for each slice\n",
        "- Get the new coefficient of each instance in a slice\n",
        "    - **Hint:** fit function defined above\n",
        "- Gather the new coefficient from each worker\n",
        "    - **Hint:** gather()\n",
        "    - Take the mean of the gathered coefficients\n",
        "- Calculate the root mean square error for the test set\n",
        "\n",
        "To know more about `scatter`, `gather` and `barrier` click [here](https://nyu-cds.github.io/python-mpi/05-collectives/)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "consistent-union",
      "metadata": {
        "id": "consistent-union",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Send the slice of data to work on to each worker\n",
        "sliced_features_X_train = comm.scatter(x_data_for_worker, root = root)\n",
        "sliced_expected_target_Y_train = comm.scatter(y_data_for_worker, root = root)\n",
        "Xm = sliced_features_X_train.values\n",
        "ym = sliced_expected_target_Y_train.values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "educated-spiritual",
      "metadata": {
        "id": "educated-spiritual",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# checking the shape of features and target received scatter\n",
        "Xm.shape, ym.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "animal-mercy",
      "metadata": {
        "id": "animal-mercy",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# fit the data with x, y to calculate the coefficients\n",
        "b0, new_coefficients = fit(Xm, ym)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "specialized-temple",
      "metadata": {
        "id": "specialized-temple",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "b0, new_coefficients"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "exterior-armor",
      "metadata": {
        "id": "exterior-armor"
      },
      "source": [
        "#### Predict the output using the new coefficients"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2tPhEfV_PLpX",
      "metadata": {
        "id": "2tPhEfV_PLpX"
      },
      "outputs": [],
      "source": [
        "# fucntion to predict the values\n",
        "def predict(x, intercept, coefficients):\n",
        "    '''\n",
        "    y = b_0 + b_1*x + ... + b_i*x_i\n",
        "    '''\n",
        "    predictions = [] # Defining empty list to store the predicted values\n",
        "    for index, row in x.iterrows(): # iterating over features\n",
        "        values = row.values # converting each row into a array\n",
        "        pred = np.multiply(values, coefficients) # multiply the coefficients with the features values\n",
        "        pred = sum(pred) # storing the sum of each features\n",
        "        pred += intercept # finally adding the intercept value\n",
        "        predictions.append(pred) # appending the values to the list\n",
        "    # returning the predictions\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "waiting-sugar",
      "metadata": {
        "id": "waiting-sugar",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "predicted_y_sliced = predict(sliced_features_X_train, b0, new_coefficients)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "legitimate-burden",
      "metadata": {
        "id": "legitimate-burden"
      },
      "source": [
        "#### Gather the new coefficients and calculate the error on train and test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "julian-yemen",
      "metadata": {
        "id": "julian-yemen",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Gather the new coeffiecient for each slice of the training data\n",
        "gather_new_coefficients = pd.DataFrame(comm.gather(new_coefficients, root=0))\n",
        "comm.barrier()\n",
        "if rank == root:\n",
        "    coef = gather_new_coefficients.mean()\n",
        "    #print(coef)\n",
        "    predicted_y = predict(X_test, intercept, coef)\n",
        "    print(\"Test set error(RMSE) is {}\" .format(rmse(Y_test.values, np.array(predicted_y))))\n",
        "    #print(\"Train set error(RMSE) is {}\" .format( rmse(Y_train.values, np.array(predicted_y_sliced))))\n",
        "t_diff = MPI.Wtime() - t_start\n",
        "print('Process {}: {} secs.' .format(rank,t_diff))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hired-uniform",
      "metadata": {
        "id": "hired-uniform"
      },
      "source": [
        "#### Exercise 17: Make a script and execute everything in one place (1 point)\n",
        "\n",
        "Write a script(.py) file which contains the code of all the above exercises in it so that you can run the code on multiple processes using MPI.\n",
        "\n",
        "**Hint:**\n",
        "\n",
        "- magic commands\n",
        "- put MPI related code under main function\n",
        "- !mpirun --allow-run-as-root -np 4 python filename.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bottom-assessment",
      "metadata": {
        "id": "bottom-assessment",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "%%writefile mlrMPI.py\n",
        "\n",
        "import pandas as pd # Importing pandas package under a name pd\n",
        "import numpy as np # Importing Numpy package under a name np\n",
        "from mpi4py import MPI # Importing MPI fro mpi4py package\n",
        "from math import sqrt # Importing sqrt function from the Math package\n",
        "from decimal import Decimal, ROUND_HALF_UP # Importing Decimal, ROUND_HALF_UP functions from the decimal package\n",
        "\n",
        "FILENAME = \"/content/PowerPlantData.csv\" # Storing File path\n",
        "# Defining a function to load the data\n",
        "def loadData(filename):\n",
        "    # Loading the dataset with column names as\n",
        "    data = pd.read_csv(filename, header=0 , names = ['AmbientTemperature', 'ExhaustVaccum', 'AmbientPressure', 'RelativeHumidity', 'EnergyOutput'])\n",
        "    # Returning the dataframe\n",
        "    return data\n",
        "# Calling the function loadData and storing the dataframe in a variable named df\n",
        "df = loadData(FILENAME)\n",
        "\n",
        " # Defining a function\n",
        "def exploreData(data):\n",
        "    print(data.shape) # Checking for number of rows and columns\n",
        "    print(data.describe()) # Summary of the data\n",
        "    print(data.isna()) # Checking for the null values in the data\n",
        "    print(sum(data.duplicated()))  # Checking for the duplicate values in the data\n",
        " # Function to handle missing data\n",
        "def handleMissingData(data):\n",
        "    data = data.dropna() # dropping the records containing null values using dropna function\n",
        "    # returning the dataframe after dropping the values\n",
        "    return data\n",
        "\n",
        "newdf = handleMissingData(df) # storing the data after removing the null values from it\n",
        "\n",
        " # Defining a function to standardization the data\n",
        "def standardizeData(dataFile):\n",
        "    # Applying standardize formula\n",
        "    dataFile = (dataFile - dataFile.mean()) / dataFile.std()\n",
        "    # returning the standardization data\n",
        "    return dataFile\n",
        "\n",
        "ScaledData = standardizeData(newdf) # Storing the data after applying normalization on the data\n",
        "\n",
        "# Function which returns features and target variables\n",
        "def FeatureSelector(data,target_name):\n",
        "    target = data[target_name] # Storing the target values\n",
        "    features = data.drop([target_name],axis=1) # Storing the features by dropping the target variable column\n",
        "    return features, target # Returning the features and target\n",
        "\n",
        "features, target = FeatureSelector(ScaledData,'EnergyOutput') # Storing the features and targets in variables respectively\n",
        "\n",
        " # Calculating the coefficients\n",
        "def estimatedCoefficients(x, y):\n",
        "    # Implementing above formula\n",
        "    xT = x.transpose() # Transpose of x\n",
        "    inversed = np.linalg.inv( xT.dot(x) ) # Inverse of a matrix\n",
        "    coefficients = inversed.dot( xT ).dot(y) # performing final dot operation\n",
        "    # Returning the coefficients\n",
        "    return coefficients\n",
        "\n",
        " # function to add dummy column into features dataframe and converting it into numpy array\n",
        "def dummyvariable(features):\n",
        "    # create a array of ones\n",
        "    m = np.ones((features.shape[0],1))\n",
        "    # combining the array of ones with features array\n",
        "    f = np.concatenate((m,features),axis=1)\n",
        "    # returning the features array\n",
        "    return f\n",
        "\n",
        " # defining a fit function\n",
        "def fit(x, y):\n",
        "    # prepare x and y values for coefficient estimates\n",
        "    x = dummyvariable(x) # adding a dummy column\n",
        "    # y = y.values\n",
        "    betas = estimatedCoefficients(x, y) # calculating the estimated coefficients\n",
        "    # intercept becomes a vector of ones\n",
        "    intercept = betas[0]\n",
        "    # coefficients becomes the rest of the betas\n",
        "    coefficients = betas[1:]\n",
        "    # returning the intercept and coefficients\n",
        "    return intercept, coefficients\n",
        "\n",
        "intercept, coefficients = fit(features, target) # fitting the data and calculating the intercept and coefficients\n",
        "\n",
        "  # function to predict the values\n",
        "def predict(x, intercept, coefficients):\n",
        "    '''\n",
        "    y = b_0 + b_1*x + ... + b_i*x_i\n",
        "    '''\n",
        "    predictions = [] # Defining empty list to store the predicted values\n",
        "    for index, row in x.iterrows(): # iterating over features\n",
        "        values = row.values # converting eaach row into a array\n",
        "        pred = np.multiply(values, coefficients) # multiply the coefficients with the features values\n",
        "        pred = sum(pred) # storing the sum of each features\n",
        "        pred += intercept # finally adding the intercept value\n",
        "        predictions.append(pred) # appending the values to the list\n",
        "    # returning the predictions\n",
        "    return predictions\n",
        "\n",
        " # function to calculate the error\n",
        "def rmse(actual, predicted):\n",
        "        # To store the value\n",
        "        sum_err = 0.0\n",
        "        # iterating over the actual values\n",
        "        for i in range(len(actual)):\n",
        "            # calculating mean squared error\n",
        "            pred_err = predicted[i] - actual[i]\n",
        "            sum_err += pred_err ** 2\n",
        "        mean_err = sum_err / float(len(actual))\n",
        "        # squaring the mean squared error to get the RMSE error value\n",
        "        return sqrt(mean_err)\n",
        "\n",
        " # Divide the data into 70% train set and 30% test set\n",
        "def train_test_split(features_X , expected_target_Y ):\n",
        "    #Randomly pick 70% 0f the data\n",
        "    set_of_data = np.random.rand(len(features_X)) <= 0.7\n",
        "    X_train = features_X[set_of_data] # Training features set\n",
        "    Y_train = expected_target_Y[set_of_data] # target train set\n",
        "    #the remaining 30% is for the test set\n",
        "    X_test  = features_X[~set_of_data] # Test features set\n",
        "    Y_test  = expected_target_Y[~set_of_data] # target test sets\n",
        "    # Returning the train features set, train targets set, test features sets, test target sets\n",
        "    return X_train, X_test, Y_train, Y_test\n",
        "\n",
        "def dividing_data(x_train, y_train, size_of_workers):\n",
        "    #Divide the data among the workers\n",
        "    slice_for_each_worker = int(Decimal(x_train.shape[0]/size_of_workers).quantize(Decimal('1.'), rounding = ROUND_HALF_UP))\n",
        "    print('Slice of data for each worker: {}'.format(slice_for_each_worker))\n",
        "    x_data_for_worker = []\n",
        "    y_data_for_worker = []\n",
        "    for i in range(0,size_of_workers):\n",
        "        if i < size_of_workers - 1:\n",
        "            x_data_for_worker.append(x_train[slice_for_each_worker*i:slice_for_each_worker*(i+1)])\n",
        "            y_data_for_worker.append(y_train[slice_for_each_worker*i:slice_for_each_worker*(i+1)])\n",
        "        else:\n",
        "            x_data_for_worker.append(x_train[slice_for_each_worker*i:])\n",
        "            y_data_for_worker.append(y_train[slice_for_each_worker*i:])\n",
        "    return x_data_for_worker, y_data_for_worker\n",
        "\n",
        "def main():\n",
        "    comm = MPI.COMM_WORLD                       # Initialize communicator\n",
        "    rank = comm.Get_rank()                        # ID of the cureent worker\n",
        "    status = MPI.Status()                       # Rank ID of sender\n",
        "    size = comm.Get_size()                      # Number odf workers\n",
        "    root = 0   # Root\n",
        "    x_data_for_worker, y_data_for_worker = [], []\n",
        "    if rank == root:\n",
        "        # Splitting the data into train and test\n",
        "        X_train, X_test, Y_train, Y_test =  train_test_split(features, target)\n",
        "        #Divide the data among the workers\n",
        "        x_data_for_worker, y_data_for_worker = dividing_data(X_train, Y_train, size)\n",
        "\n",
        "    # Send the slice of data to work on to each worker\n",
        "    sliced_features_X_train = comm.scatter(x_data_for_worker, root = root)\n",
        "    sliced_expected_target_Y_train = comm.scatter(y_data_for_worker, root = root)\n",
        "    Xm = sliced_features_X_train.values\n",
        "    ym = sliced_expected_target_Y_train.values\n",
        "    # fit the data with x, y to calculate the coefficients\n",
        "    b0, new_coefficients = fit(Xm, ym)\n",
        "    ##predicted_y_sliced = predict(sliced_features_X_train, b0, new_coefficients)\n",
        "    # Gather the new coeffiecient for each slice of the training data\n",
        "    gather_new_coefficients = pd.DataFrame(comm.gather(new_coefficients, root=0))\n",
        "    comm.barrier()\n",
        "    if rank == root:\n",
        "        coef = gather_new_coefficients.mean()\n",
        "        #print(coef)\n",
        "        predicted_y = predict(X_test, intercept, coef)\n",
        "        print(\"Test set error(RMSE) is {}\" .format(rmse(Y_test.values, np.array(predicted_y))))\n",
        "\n",
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "experimental-waterproof",
      "metadata": {
        "id": "experimental-waterproof",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "!mpirun --allow-run-as-root -np 4 python mlrMPI.py"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mhPRoQhL-OVu",
      "metadata": {
        "id": "mhPRoQhL-OVu"
      },
      "source": [
        "Note: In case any issue is encountered while executing MPI file then please copy only the MPI related code in the new notebook and execute."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "protecting-assets",
      "metadata": {
        "id": "protecting-assets"
      },
      "source": [
        "#### Exercise 18: Use Sklearn to compare (1 point)\n",
        "\n",
        "Apply the Linear regression on the given data using sklearn package and compare with the above results\n",
        "\n",
        "**Hint:**\n",
        "* Split the data into train and test\n",
        "* Fit the train data and predict the test data using `sklearn Linear Regression`\n",
        "* Compare the coefficients and intercept with above estimated coefficients\n",
        "* calculate loss (RMSE) on test data and predictions and compare"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "alternate-constraint",
      "metadata": {
        "id": "alternate-constraint",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "xtrain, xtest, ytrain, ytest = train_test_split(features, target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "consecutive-zealand",
      "metadata": {
        "id": "consecutive-zealand",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "reg = LinearRegression()\n",
        "reg.fit(xtrain, ytrain)\n",
        "predictions = reg.predict(xtest)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "compound-papua",
      "metadata": {
        "id": "compound-papua",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# coefficients from sklearn Linear Regression\n",
        "print(reg.coef_)\n",
        "# intercept from sklearn Linear Regression\n",
        "print(reg.intercept_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sophisticated-attention",
      "metadata": {
        "id": "sophisticated-attention",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "print(sqrt(mean_squared_error(predictions, ytest)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.7rc1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}