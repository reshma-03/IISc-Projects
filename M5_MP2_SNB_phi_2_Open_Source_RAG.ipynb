{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reshma-03/IISc-Projects/blob/main/M5_MP2_SNB_phi_2_Open_Source_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Advanced Certification Program in Computational Data Science\n",
        "## A programme by IISc and TalentSprint\n",
        "### Mini-Project: Open Source Retrieval Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "i5gB6iG94kzV"
      },
      "id": "i5gB6iG94kzV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**DISCLAIMER:** THIS NOTEBOOK IS PROVIDED ONLY AS A REFERENCE SOLUTION NOTEBOOK FOR THE MINI-PROJECT. THERE MAY BE OTHER POSSIBLE APPROACHES/METHODS TO ACHIEVE THE SAME RESULTS."
      ],
      "metadata": {
        "id": "mIcSvfa84v5I"
      },
      "id": "mIcSvfa84v5I"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Problem Statement\n",
        "\n",
        "Retrieval Q and A Integrated with LLM"
      ],
      "metadata": {
        "id": "I0FW5b1NQ_Wu"
      },
      "id": "I0FW5b1NQ_Wu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Learning Objectives\n",
        "\n",
        "At the end of the experiment you will be able to :\n",
        "\n",
        "1. Run Phi-2, Microsoft's small language model (SLM), using two methods:\n",
        "   - Direct Inference using HuggingFace\n",
        "   - Retrieval Augmented Generation (RAG) using Llama-index\n",
        "2. Know the basic working of Llama Index VectorStore\n",
        "3. Implement the Hugging Face embedding\n",
        "4. Implement a simple FAISS-based vector store for efficient similarity search of high-dimensional data.\n",
        "5. Create RetrievalQA chain along with prompt template\n",
        "6. Compare the **effectiveness of Phi-2 & Zephyr-7b-beta model** by means of Cosine Similarity.\n",
        "7. Compare the **effectiveness of 5 different Hugging Face embeddings** by computing and analyzing the cosine similarity between the embedded vectors of queries and results from Zephyr-7b-beta model, to understand the differences in semantic similarity and performance.\n"
      ],
      "metadata": {
        "id": "1jylUQmEIH-y"
      },
      "id": "1jylUQmEIH-y"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Information\n",
        "\n",
        "Retrieval Augmented Generation (RAG) combines the advanced text-generation capabilities of GPT and other large language models with information retrieval functions to provide precise and contextually relevant information. This innovative approach improves language models' ability to understand and process user queries by integrating the latest and most relevant data. As RAG continues to evolve, its growing applications are set to revolutionize AI efficiency and utility.\n",
        "\n",
        "##Retrieval-Augmented Generation (RAG) Process\n",
        "###  **Feeding LLMs with Accurate Information**:\n",
        "\n",
        "- Instead of directly querying the language model, relevant data is first retrieved from a well-maintained knowledge library.\n",
        "\n",
        "\n",
        "###**Retrieval Before Generation**:\n",
        "\n",
        "- Accurate data is retrieved using vector embeddings (numerical representations of the data).\n",
        "- These embeddings help match the query with relevant documents in a vector database.\n",
        "\n",
        "\n",
        "###**Context for Generation**:\n",
        "\n",
        "- Once the requested document or information is found, the retrieved context is used by the model to generate the answer.\n",
        "\n",
        "\n",
        "###**Reduces Hallucinations**:\n",
        "\n",
        "- This approach lowers the risk of hallucinations, where the model generates inaccurate or false information.\n",
        "\n",
        "\n",
        "###**No Need for Retraining**:\n",
        "\n",
        "- The knowledge base can be updated without retraining the model, making the system adaptable without incurring high costs.\n",
        "\n",
        "\n",
        "###**Cost-Effective Model Updates**:\n",
        "\n",
        "- By using a retriever system, models can be updated dynamically without the expense of a full model retraining process.\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\" https://cdn.exec.talentsprint.com/static/cds/RAG_Image.jpg\" width= 600 px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "RAG brings together four key components:\n",
        "\n",
        "- **Embedding model**: This is where documents are turned into vectors, or numerical representations, which make it easier for the system to manage and compare large amounts of text data.\n",
        "- **Retriever**: Think of this as the search engine within RAG. It uses the embedding model to process a question and fetch the most relevant document vectors that match the query.\n",
        "- **Reranker (optional)**: This component takes things a step further by evaluating the retrieved documents to determine how relevant they are to the question at hand, providing a relevance score for each one.\n",
        "- **Language model**: Finally, this part of the system takes the top documents provided by the retriever or reranker, along with the original question, and crafts a precise answer.\n",
        "To know more about the RAG, refer [here](https://www.superannotate.com/blog/rag-explained)."
      ],
      "metadata": {
        "id": "Vi22cd57RXms"
      },
      "id": "Vi22cd57RXms"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "In this notebook, we'll explore how to run Phi-2, Microsoft's small language model (SLM), using two methods:\n",
        "- Direct Inference using HuggingFace\n",
        "- Retrieval Augmented Generation (RAG) using Llama-index\n",
        "\n",
        "Phi-2 is an SLM with 2.7 billion parameters and trained on 1.4T tokens."
      ],
      "metadata": {
        "id": "lt_QVg04RLot"
      },
      "id": "lt_QVg04RLot"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDCe4r5fwDtD"
      },
      "source": [
        "## Benefits of Small Models\n",
        "- Fast fine-tuning\n",
        "- Can be run locally\n",
        "- Requires less computational resources\n",
        "\n",
        "### **Note: This notebook has to necessarily run on GPU.**"
      ],
      "id": "xDCe4r5fwDtD"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grading = 10 Points"
      ],
      "metadata": {
        "id": "dIu0egmxkROS"
      },
      "id": "dIu0egmxkROS"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oXWCjoVEwDtD"
      },
      "source": [
        "## Install Required Packages\n",
        "Install necessary libraries for running Phi-2 on Google Colab."
      ],
      "id": "oXWCjoVEwDtD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H1QOUdb2wDtE"
      },
      "outputs": [],
      "source": [
        "!pip -qq install langchain torch transformers sentencepiece accelerate bitsandbytes einops sentence-transformers\n",
        "!pip -qq install langchain_community\n",
        "!pip -qq install langchain_huggingface\n",
        "!pip -qq install huggingface_hub\n",
        "!pip -qq install chromadb"
      ],
      "id": "H1QOUdb2wDtE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing necessary packages"
      ],
      "metadata": {
        "id": "5sUNWYCUWIMW"
      },
      "id": "5sUNWYCUWIMW"
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from getpass import getpass\n",
        "from langchain import hub\n",
        "from langchain.llms import HuggingFacePipeline\n",
        "from transformers import pipeline\n",
        "from langchain_community.llms import HuggingFaceEndpoint\n",
        "from langchain_community.chat_models.huggingface import ChatHuggingFace\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "from langchain_community.embeddings import HuggingFaceHubEmbeddings\n",
        "from langchain.prompts import PromptTemplate\n",
        "from transformers import pipeline\n",
        "\n",
        "from langchain.llms import HuggingFaceHub\n",
        "from langchain import LLMChain\n",
        "from langchain.chains import RetrievalQA\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "#from llama_index.embeddings import HuggingFaceEmbedding\n",
        "\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "import torch\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "13z90ZwCWCMg"
      },
      "id": "13z90ZwCWCMg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase-I:** Comparison between Microsoft Phi-2 and Hugging Face Zephyr-7b-beta without Retrieval Augmented Generation (RAG)"
      ],
      "metadata": {
        "id": "Wm6u9sf8dpcU"
      },
      "id": "Wm6u9sf8dpcU"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zg2qL17QwDtF"
      },
      "source": [
        "## 1.1 Load the Phi-2 Model and Tokenizer to integrate with Langchain using HuggingFace Pipeline\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\" https://cdn.exec.talentsprint.com/static/cds/content/Phi_2_without_RAG-1.png\" width= 600 px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "**Exercise-1:** Load Phi-2 model and tokenizer from Huggingface and create a pipeline for text generation. Then integrate the Phi-2 model with Langchain for better prompt handling. **(0.5 point)**"
      ],
      "id": "Zg2qL17QwDtF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8BDZfrVYwDtF"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, pipeline, AutoModelForCausalLM\n",
        "from langchain import HuggingFacePipeline\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "# Get model's tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    'microsoft/phi-2',\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Load the model\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    'microsoft/phi-2',\n",
        "    torch_dtype='auto',\n",
        "    device_map='auto',\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Create a text-generation pipeline\n",
        "text_gen_pipeline = transformers.pipeline(\n",
        "    'text-generation',\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    device_map='auto',\n",
        "    max_new_tokens=256,\n",
        "    temperature=0.5\n",
        ")"
      ],
      "id": "8BDZfrVYwDtF"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcMLV9b0wDtG"
      },
      "source": [
        "Integrating the Phi-2 model with Langchain for better prompt handling."
      ],
      "id": "xcMLV9b0wDtG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAlwk0iXwDtG"
      },
      "outputs": [],
      "source": [
        "from langchain import HuggingFacePipeline\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains.llm import LLMChain\n",
        "\n",
        "# Creating a Text-Generation Pipeline Using Hugging Face Transformers\n",
        "phi2_HFP_llm = HuggingFacePipeline(pipeline=text_gen_pipeline)\n",
        "text_gen_pipeline.model.config.pad_token_id = text_gen_pipeline.model.config.eos_token_id\n",
        "\n",
        "# Define a prompt template\n",
        "task_template = '''\n",
        "You are a friendly chatbot assistant that gives structured output.\n",
        "Your role is to arrange the given task in this structure.\n",
        "### instruction:\n",
        "{instruction}\n",
        "Output:\n",
        "'''\n",
        "\n",
        "# Creating a Task Prompt Template and LLM Chain Using phi2 Model\n",
        "task_prompt_template = PromptTemplate(input_variables=['instruction'], template=task_template)\n",
        "phi2_HFP_llm_chain = LLMChain(prompt=task_prompt_template, llm=phi2_HFP_llm)"
      ],
      "id": "rAlwk0iXwDtG"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGh5g1fawDtG"
      },
      "source": [
        "## 1.3 Querying the Phi-2 Model\n",
        "**Exercise-2:** Now let's query the model with a prompt. For example, let's ask the model to 'Give an overview of Computational Data Science PG Level certificaion course'. From the response, extract the 'text' field and save it in a variable 'phi_2_extracted_output'. **(0.5 point)**"
      ],
      "id": "PGh5g1fawDtG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rz6R7S4nwDtH"
      },
      "outputs": [],
      "source": [
        "# Example query\n",
        "question = 'Give an overview of Computational Data Science PG Level certificaion course'\n",
        "\n",
        "response = phi2_HFP_llm_chain.invoke(question)\n",
        "print(response)\n"
      ],
      "id": "rz6R7S4nwDtH"
    },
    {
      "cell_type": "code",
      "source": [
        "# Simulated response from the model\n",
        "# response = {\n",
        "#     'instruction': 'Give an overview of Computational Data Science PG Level certificaion course',\n",
        "#     'text': '\\nYou are a friendly chatbot assistant that gives structured output.\\nYour role is to arrange the given task in this structure.\\n### instruction:\\nGive an overview of Computational Data Science PG Level certificaion course\\nOutput:\\nThe Computational Data Science PG Level Certification course is designed to provide an overview of the field of computational data science.\\n'\n",
        "# }\n",
        "\n",
        "# Extract the 'text' field from the response\n",
        "output_text = response['text']\n",
        "\n",
        "# Parse the text to get the output part only\n",
        "# Assuming the output starts after the keyword \"Output:\"\n",
        "output_start = output_text.find(\"Output:\") + len(\"Output:\")  # Find the index after \"Output:\"\n",
        "phi_2_extracted_output = output_text[output_start:].strip()  # Extract the output part and strip extra whitespace\n",
        "\n",
        "print(\"Extracted Output:\", phi_2_extracted_output)"
      ],
      "metadata": {
        "id": "cRvj0kTLcH5X"
      },
      "id": "cRvj0kTLcH5X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1.4 Using the HuggingFace API Key"
      ],
      "metadata": {
        "id": "07y4h_tAWrq2"
      },
      "id": "07y4h_tAWrq2"
    },
    {
      "cell_type": "code",
      "source": [
        "h_api_key = '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' # Your Hugging Face API Key"
      ],
      "metadata": {
        "id": "wooEmPPIW3Bh"
      },
      "id": "wooEmPPIW3Bh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set your HuggingFace API key\n",
        "os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = h_api_key"
      ],
      "metadata": {
        "id": "b5lOv1YiW6R4"
      },
      "id": "b5lOv1YiW6R4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.5 Initializing HuggingFaceEndpoint with [**HuggingFaceH4/zephyr-7b-beta**](https://huggingface.co/HuggingFaceH4/zephyr-7b-beta) model\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\" https://cdn.exec.talentsprint.com/static/cds/content/zephyr_without_RAG-2.png\" width= 600 px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n"
      ],
      "metadata": {
        "id": "oWm_gy3JXIwP"
      },
      "id": "oWm_gy3JXIwP"
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize HuggingFaceEndpoint with your endpoint URL\n",
        "endpoint_url = \"https://huggingface.co/HuggingFaceH4/zephyr-7b-beta\"\n",
        "\n",
        "# Initialize the model name \"HuggingFaceH4/zephyr-7b-beta\" in a variable model_name\n",
        "model_name = \"HuggingFaceH4/zephyr-7b-beta\""
      ],
      "metadata": {
        "id": "0wJjL6m4XNI5"
      },
      "id": "0wJjL6m4XNI5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.6 Creating the LLM using zephyr-7b-beta\n",
        "\n",
        "**Exercise-3:** Create an LLM using HuggingFaceEndpoint. **(0.5 point)**"
      ],
      "metadata": {
        "id": "aEAwW1nsXVur"
      },
      "id": "aEAwW1nsXVur"
    },
    {
      "cell_type": "code",
      "source": [
        "# Import HuggingFace model abstraction class from langchain\n",
        "from langchain_huggingface import HuggingFaceEndpoint"
      ],
      "metadata": {
        "id": "-dFhLKSQXbR_"
      },
      "id": "-dFhLKSQXbR_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an LLM using HuggingFaceEndpoint\n",
        "zephyr_7b_beta_HFE_llm = HuggingFaceEndpoint(\n",
        "    repo_id=model_name,\n",
        "    task=\"text-generation\",\n",
        "    max_new_tokens = 512,\n",
        "    top_k = 30,\n",
        "    huggingfacehub_api_token=h_api_key,\n",
        "    temperature = 0.1,\n",
        "    repetition_penalty = 1.03\n",
        ")"
      ],
      "metadata": {
        "id": "gtKzNe-sXfaI"
      },
      "id": "gtKzNe-sXfaI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.7 Querying the HuggingFace zephyr-7b-beta Model\n",
        "Now let's query the model with a prompt. For example, let's ask the model to give an overview of the Computational Data Science PG Level certification course."
      ],
      "metadata": {
        "id": "iWcA098TX1ZP"
      },
      "id": "iWcA098TX1ZP"
    },
    {
      "cell_type": "code",
      "source": [
        "zephyr_7b_beta_response = zephyr_7b_beta_HFE_llm.invoke(\"Give an overview of Computational Data Science PG Level certificaion course\")\n",
        "print(zephyr_7b_beta_response)"
      ],
      "metadata": {
        "id": "e_7uZ_X6Xk4J"
      },
      "id": "e_7uZ_X6Xk4J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1.8 Comparison: Microsoft Phi-2 and Hugging Face zephyr-7b-beta model\n",
        "\n",
        "**Exercise-4:** Compare the RetrievalQA performance between Phi-2 and Hugging Face and zephyr-7b-beta model using Cosine Similarity. **(0.5 point)**\n",
        "\n",
        "- **(a)** Consider the reference Question: 'Give an overview of Computational Data Science PG Level certificaion course'. Compute Cosine Similarity.\n",
        "\n",
        "- **(b)** Consider the Benchmark_solution: 'Are you a working professional looking to build expertise in Data Science? Look no further than the PG Level Advanced Certification course in\n",
        "Data Science offered by Indian Institute of Science (IISc) in association with TalentSprint. This highly sought-after programme offers a unique 5-step learning process, including LIVE online faculty-led interactive sessions, capstone projects, mentorship, case studies, and data stories. Taught by world-class faculty from a global institution and supplemented with industry learnings, this 12-month programme is best suited for professionals who want to gain practical hands-on experience in solving real-life challenges. The programme teaches participants how to build powerful models to generate actionable insights, necessary for making data-driven decisions. With an overwhelming response, this programme has enabled 750+ professionals to build Data Science expertise. Don't miss the opportunity to gain an in-depth understanding of the mechanics of working with data and identifying insights. Enroll now and take your career to the next level with the PG Level Advanced Certification course in Computational Data Science.' Compute Cosine Similarity."
      ],
      "metadata": {
        "id": "PWXRAKAQZuXn"
      },
      "id": "PWXRAKAQZuXn"
    },
    {
      "cell_type": "code",
      "source": [
        "# (a)\n",
        "Q1 = \"Give an overview of Computational Data Science PG Level certificaion course\"\n",
        "h_embeddings = HuggingFaceEmbeddings()\n",
        "Q1_e = np.array(h_embeddings.embed_query(Q1)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "\n",
        "phi_2_e = np.array(h_embeddings.embed_query(phi_2_extracted_output)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "\n",
        "zephyr_7b_beta_e = np.array(h_embeddings.embed_query(zephyr_7b_beta_response)).reshape(1, -1)  # Convert to array and reshape to 2D"
      ],
      "metadata": {
        "id": "uqchYQNgaIso"
      },
      "id": "uqchYQNgaIso",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cosine similarity\n",
        "cosine_sim_phi_2 = cosine_similarity(Q1_e, phi_2_e)[0][0]\n",
        "cosine_sim_zephyr_7b_beta = cosine_similarity(Q1_e, zephyr_7b_beta_e)[0][0]\n",
        "\n",
        "print(f\"Cosine Similarity between Q1 and phi_2_extracted_output: {cosine_sim_phi_2}\")\n",
        "print(f\"Cosine Similarity between Q1 and zephyr_7b_beta_response: {cosine_sim_zephyr_7b_beta}\")"
      ],
      "metadata": {
        "id": "2zrPzfkgdl6f"
      },
      "id": "2zrPzfkgdl6f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (b)\n",
        "Benchmark_solution = \"\"\"Are you a working professional looking to build expertise in Data Science?\n",
        "Look no further than the PG Level Advanced Certification course in Data Science offered by Indian Institute of Science (IISc)\n",
        "in association with NSE TalentSprint. This highly sought-after programme offers a unique 5-step learning process, including\n",
        "LIVE online faculty-led interactive sessions, capstone projects, mentorship, case studies, and data stories.\n",
        "Taught by world-class faculty from a global institution and supplemented with industry learnings, this 12-month programme is best suited\n",
        "for professionals who want to gain practical hands-on experience in solving real-life challenges. The programme teaches participants\n",
        "how to build powerful models to generate actionable insights, necessary for making data-driven decisions.\n",
        "With an overwhelming response, this programme has enabled 750+ professionals to build Data Science expertise.\n",
        "Don't miss the opportunity to gain an in-depth understanding of the mechanics of working with data and identifying insights.\n",
        "Enroll now and take your career to the next level with the PG Level Advanced Certification course in Computational Data Science.\"\"\"\n",
        "\n",
        "BMS = Benchmark_solution\n",
        "h_embeddings = HuggingFaceEmbeddings()\n",
        "BMS_e = np.array(h_embeddings.embed_query(BMS)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "\n",
        "phi_2_e = np.array(h_embeddings.embed_query(phi_2_extracted_output)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "\n",
        "zephyr_7b_beta_e = np.array(h_embeddings.embed_query(zephyr_7b_beta_response)).reshape(1, -1)  # Convert to array and reshape to 2D"
      ],
      "metadata": {
        "id": "B-c_hfSvvMDm"
      },
      "id": "B-c_hfSvvMDm",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cosine similarity\n",
        "cosine_sim_phi_2 = cosine_similarity(BMS_e, phi_2_e)[0][0]\n",
        "cosine_sim_zephyr_7b_beta = cosine_similarity(BMS_e, zephyr_7b_beta_e)[0][0]\n",
        "\n",
        "print(f\"Cosine Similarity between BMS and phi_2_extracted_output: {cosine_sim_phi_2}\")\n",
        "print(f\"Cosine Similarity between BMS and zephyr_7b_beta_response: {cosine_sim_zephyr_7b_beta}\")"
      ],
      "metadata": {
        "id": "EpWLckZ7wwGf"
      },
      "id": "EpWLckZ7wwGf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cac1XHwwDtH"
      },
      "source": [
        "#**Phase-II:** Performing Retrieval Augmented Generation (RAG) with Microsoft Phi-2\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\" https://cdn.exec.talentsprint.com/static/cds/content/Phi_2_with_RAG-3.png\" width= 1200 px/>\n",
        "</center>\n",
        "<br><br>\n",
        "\n",
        "## 2.1 Retrieval Augmented Generation (RAG) with Llama-index\n",
        "\n",
        "In this section, we'll implement RAG using Llama-index to augment the retrieval from document data."
      ],
      "id": "5cac1XHwwDtH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YKZ7geLzwDtH"
      },
      "outputs": [],
      "source": [
        "!pip install -q pypdf llama-index python-dotenv"
      ],
      "id": "YKZ7geLzwDtH"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0455w-K7wDtI"
      },
      "source": [
        "## 2.2 Setup Llama-index\n",
        "Load necessary components, read documents, and set up the RAG pipeline."
      ],
      "id": "0455w-K7wDtI"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -qq install --upgrade llama-index\n",
        "!pip -qq install llama-index-embeddings-langchain\n",
        "!pip -qq install llama_index.llms.ollama\n",
        "!pip -qq install llama_index.embeddings.huggingface\n",
        "!pip -qq install llama-index-llms-langchain\n",
        "!pip -qq install faiss-gpu"
      ],
      "metadata": {
        "id": "YcNDk7YXVAyD"
      },
      "id": "YcNDk7YXVAyD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.3 Importing necessary packages from Llama-index"
      ],
      "metadata": {
        "id": "tFKcnzQMwuLg"
      },
      "id": "tFKcnzQMwuLg"
    },
    {
      "cell_type": "code",
      "source": [
        "from llama_index.core.indices.vector_store.base import VectorStoreIndex\n",
        "from llama_index.core import SimpleDirectoryReader\n",
        "from langchain.vectorstores import FAISS\n",
        "from llama_index.core import ServiceContext"
      ],
      "metadata": {
        "id": "3zwvTV9Sg5Xk"
      },
      "id": "3zwvTV9Sg5Xk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 2.4 Download Dataset\n",
        "#!wget -qq https://cdn.exec.talentsprint.com/static/cds/content/pca_d1.pdf\n",
        "#!wget -qq https://cdn.exec.talentsprint.com/static/cds/content/ens_d2.pdf\n",
        "!wget -qq https://cdn.exec.talentsprint.com/static/cds/content/demo_faqs.csv\n",
        "!wget -qq https://cdn.exec.talentsprint.com/static/cds/content/docs.zip\n",
        "!unzip docs.zip -d docs  # This will unzip docs.zip into a folder named 'docs'\n",
        "print(\"Dataset downloaded successfully!!\")"
      ],
      "metadata": {
        "cellView": "form",
        "id": "iIlzQjWbjxsN"
      },
      "id": "iIlzQjWbjxsN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.5 Load Data (PDF Document)"
      ],
      "metadata": {
        "id": "QSDW75lduXIS"
      },
      "id": "QSDW75lduXIS"
    },
    {
      "cell_type": "code",
      "source": [
        "# Read documents\n",
        "documents = SimpleDirectoryReader('/content/docs/docs').load_data()\n",
        "documents"
      ],
      "metadata": {
        "id": "uRWwT8Y47GQd"
      },
      "id": "uRWwT8Y47GQd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.6 Creating the Embedding Model using HuggingFaceEmbeddings **'BAAI/bge-small-en-v1.5'**\n",
        "\n",
        "**Exercise-5:** Define an embedding model using HuggingFaceEmbeddings 'BAAI/bge-small-en-v1.5'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "_vb3QoueuI1n"
      },
      "id": "_vb3QoueuI1n"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define embedding model\n",
        "embed_model = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')"
      ],
      "metadata": {
        "id": "g02xx49Ino8s"
      },
      "id": "g02xx49Ino8s",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.7 Create a Vector Store using VectorStoreIndex\n",
        "\n",
        "**Exercise-6:** Create the vector index and vector store from documents using the embedding model (used in Exercise-5). **(0.5 point)**"
      ],
      "metadata": {
        "id": "Ty7m5RcvvX3z"
      },
      "id": "Ty7m5RcvvX3z"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the vector index from documents using the embedding model\n",
        "index = VectorStoreIndex.from_documents(documents, embed_model=embed_model)\n",
        "index"
      ],
      "metadata": {
        "id": "UzoSxoe31pz3"
      },
      "id": "UzoSxoe31pz3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the vector store from documents using the embedding model and vector index\n",
        "vector_store = VectorStoreIndex.from_documents(documents, embed_model=embed_model, faiss_index=index)\n",
        "vector_store"
      ],
      "metadata": {
        "id": "2K5HE2l4vpvM"
      },
      "id": "2K5HE2l4vpvM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.8 Create Query Engines and Test the RAG Pipeline\n",
        "\n",
        "**Exercise-7:** Create a Query Engine by using 'as_query_engine()' and then test the RAG pipeline for the Query: 'Give an overview of Computational Data Science PG Level certificaion course'. From the response, extract the text part and save it in a variable 'answer_text'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "a28wIwrDwTgy"
      },
      "id": "a28wIwrDwTgy"
    },
    {
      "cell_type": "code",
      "source": [
        "# ... until you create a query engine\n",
        "query_engine = index.as_query_engine(llm=phi2_HFP_llm)"
      ],
      "metadata": {
        "id": "_fjvU0Sswbur"
      },
      "id": "_fjvU0Sswbur",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run a sample query to test the RAG pipeline."
      ],
      "metadata": {
        "id": "jTdmu21ew9ew"
      },
      "id": "jTdmu21ew9ew"
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the RAG pipeline\n",
        "response = query_engine.query('Give an overview of Computational Data Science PG Level certificaion course')\n",
        "result_text = response.response\n",
        "print(result_text)"
      ],
      "metadata": {
        "id": "tl8Pzu7CBGOb"
      },
      "id": "tl8Pzu7CBGOb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the part after \"Answer:\"\n",
        "answer_start = result_text.find(\"Answer:\")  # Find the index where \"Answer:\" starts\n",
        "if answer_start != -1:\n",
        "    answer_text = result_text[answer_start + len(\"Answer:\"):].strip()  # Extract the part after \"Answer:\"\n",
        "    print(\"Extracted Answer:\\n\", answer_text)\n",
        "else:\n",
        "    print(\"Answer section not found.\")"
      ],
      "metadata": {
        "id": "2TatHqLZXLe3"
      },
      "id": "2TatHqLZXLe3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.9 RAG Performance Evaluation using Cosine Similarity\n",
        "\n",
        "**Exercise-8:** Measure the RAG performance using Cosine Similarity. **(0.5 point)**\n",
        "\n",
        "- **(a)** Consider the reference Question: 'Give an overview of Computational Data Science PG Level certificaion course'. Calculate the Cosine Similarity.\n",
        "- **(b)** Consider the Benchmark_solution [as considered in Exercise-4 (b)]. Calculate the Cosine Similarity."
      ],
      "metadata": {
        "id": "spvWWQOq4ONe"
      },
      "id": "spvWWQOq4ONe"
    },
    {
      "cell_type": "code",
      "source": [
        "# (a)\n",
        "Q1 = \"Give an overview of Computational Data Science PG Level certificaion course\"\n",
        "h_embeddings = embed_model\n",
        "Q1_e = np.array(h_embeddings.embed_query(Q1)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "\n",
        "RAG_with_phi_2_e = np.array(h_embeddings.embed_query(answer_text)).reshape(1, -1)  # Convert to array and reshape to 2D"
      ],
      "metadata": {
        "id": "cNYOGEC94wbU"
      },
      "id": "cNYOGEC94wbU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cosine similarity\n",
        "#cosine_sim_phi_2 = cosine_similarity(Q1_e, phi_2_e)[0][0]\n",
        "#cosine_sim_zephyr_7b_beta = cosine_similarity(Q1_e, zephyr_7b_beta_e)[0][0]\n",
        "cosine_sim_RAG_with_phi_2 = cosine_similarity(Q1_e, RAG_with_phi_2_e)[0][0]\n",
        "\n",
        "print(f\"Cosine Similarity between Q1 and RAG response: {cosine_sim_RAG_with_phi_2}\")\n",
        "#print(f\"Cosine Similarity between Q1 and zephyr_7b_beta_response: {cosine_sim_zephyr_7b_beta}\")"
      ],
      "metadata": {
        "id": "Oq3aYcRd5jm-"
      },
      "id": "Oq3aYcRd5jm-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Cosine Similarity between Q1 and phi_2_extracted_output: 0.868318242097743\n",
        "- Cosine Similarity between Q1 and zephyr_7b_beta_response: 0.7777683843843852\n",
        "- Cosine Similarity between Q1 and RAG response: 0.8078316935786811\n",
        "\n",
        "**So considering the reference query Q1, we can observe from the above value, that the Cosine Similarity is 80.783% by using RAG Architecture with Microsoft Phi-2 model.**"
      ],
      "metadata": {
        "id": "5stjSzpm6Cvc"
      },
      "id": "5stjSzpm6Cvc"
    },
    {
      "cell_type": "code",
      "source": [
        "# (b)\n",
        "BMS = Benchmark_solution\n",
        "h_embeddings = embed_model\n",
        "BMS_e = np.array(h_embeddings.embed_query(BMS)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "\n",
        "RAG_with_phi_2_e = np.array(h_embeddings.embed_query(answer_text)).reshape(1, -1)  # Convert to array and reshape to 2D"
      ],
      "metadata": {
        "id": "_jT8qYssyHmH"
      },
      "id": "_jT8qYssyHmH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cosine similarity\n",
        "#cosine_sim_phi_2 = cosine_similarity(Q1_e, phi_2_e)[0][0]\n",
        "#cosine_sim_zephyr_7b_beta = cosine_similarity(Q1_e, zephyr_7b_beta_e)[0][0]\n",
        "cosine_sim_RAG_with_phi_2 = cosine_similarity(BMS_e, RAG_with_phi_2_e)[0][0]\n",
        "\n",
        "print(f\"Cosine Similarity between BMS and RAG response: {cosine_sim_RAG_with_phi_2}\")\n",
        "#print(f\"Cosine Similarity between Q1 and zephyr_7b_beta_response: {cosine_sim_zephyr_7b_beta}\")"
      ],
      "metadata": {
        "id": "_Kreb5M8ycpT"
      },
      "id": "_Kreb5M8ycpT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So considering the Benchmark_solution BMS, we can observe from the above value, that the Cosine Similarity is 83.162% by using RAG Architecture with Microsoft Phi-2 model.**"
      ],
      "metadata": {
        "id": "PjT0uzJHypCC"
      },
      "id": "PjT0uzJHypCC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase-III:** Performing RAG using HuggingFace Retrieval Chain For 5 different Embedding models and FAISS Vector Store\n",
        "We will use CSV Dataset for this phase.\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\" https://cdn.exec.talentsprint.com/static/cds/content/varying_embeddings-4.png\" height = 600 width= 1600 px/>\n",
        "</center>\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "Ls6NTwqIDP7T"
      },
      "id": "Ls6NTwqIDP7T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Load Data (CSV Dataset)"
      ],
      "metadata": {
        "id": "dyRf60DgFU0l"
      },
      "id": "dyRf60DgFU0l"
    },
    {
      "cell_type": "code",
      "source": [
        "loader = CSVLoader(file_path='/content/demo_faqs.csv', source_column=\"prompt\",encoding='latin-1')\n",
        "\n",
        "# Store the loaded data in the 'data' variable\n",
        "data = loader.load()\n",
        "documents_csv = data"
      ],
      "metadata": {
        "id": "wRLA-EQ-FGpp"
      },
      "id": "wRLA-EQ-FGpp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Using 5 different HuggungFace Embedding Models"
      ],
      "metadata": {
        "id": "FIl3EmPhXHK9"
      },
      "id": "FIl3EmPhXHK9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define embedding model-1\n",
        "embed_model_1 = HuggingFaceEmbeddings(model_name='BAAI/bge-small-en-v1.5')\n",
        "\n",
        "# Define embedding model-2\n",
        "embed_model_2 = HuggingFaceEmbeddings(model_name='sentence-transformers/all-MiniLM-L6-v2')\n",
        "\n",
        "# Define embedding model-3\n",
        "embed_model_3 = HuggingFaceEmbeddings(model_name='sentence-transformers/paraphrase-MiniLM-L12-v2')\n",
        "\n",
        "# Define embedding model-4\n",
        "embed_model_4 = HuggingFaceEmbeddings(model_name='sentence-transformers/all-distilroberta-v1')\n",
        "\n",
        "# Define embedding model-5\n",
        "embed_model_5 = HuggingFaceEmbeddings(model_name='sentence-transformers/multi-qa-MiniLM-L6-cos-v1')"
      ],
      "metadata": {
        "id": "XZVk85YQXZDN"
      },
      "id": "XZVk85YQXZDN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.3 Vector store using FAISS\n",
        "\n",
        "Facebook AI Similarity Search (FAISS) is a library for efficient similarity search and clustering of dense vectors. It contains algorithms that search in sets of vectors of any size, up to ones that possibly do not fit in RAM. It also contains supporting code for evaluation and parameter tuning.\n",
        "For further details, please refer to the [link](https://faiss.ai/)\n",
        "\n",
        "How to use functionality related to the FAISS vector database?\n",
        "\n",
        "In the following code cell, we will show functionality specific to this integration. After going through, it may be useful to explore relevant to learn how to use this vectorstore as part of a larger chain.\n",
        "\n",
        "**Exercise-9:** Create a FAISS vector database using Hugging Face Embeddings model 'BAAI/bge-small-en-v1.5'. Then retrieve relevant answers for a query. Use 'get_relevant_documents()'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "uf7zGpKPlq5q"
      },
      "id": "uf7zGpKPlq5q"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS instance for vector database from 'data'\n",
        "h_vectordb_1 = FAISS.from_documents(documents=data,\n",
        "                                 embedding=embed_model_1)\n",
        "\n",
        "# Create a retriever for querying the vector database\n",
        "h_retriever_1 = h_vectordb_1.as_retriever(score_threshold = 0.7)"
      ],
      "metadata": {
        "id": "ljLHwiZFlzs_"
      },
      "id": "ljLHwiZFlzs_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code cell, The provided code snippet sets up a FAISS (Facebook AI Similarity Search) vector database to store document embeddings and enables querying this database using a retriever with a specific score threshold.\n",
        "\n",
        "- **FAISS.from_documents(...)**: This method initializes a FAISS vector database using a list of documents and a pre-defined embedding model.\n",
        "- **h_vectordb.as_retriever(...)**: This method converts the FAISS vector database into a retriever object that can be queried using natural language or embedded queries."
      ],
      "metadata": {
        "id": "qZDaBYVnulVo"
      },
      "id": "qZDaBYVnulVo"
    },
    {
      "cell_type": "code",
      "source": [
        "h_rdocs_1 = h_retriever_1.get_relevant_documents(\"how about job placement support?\")\n",
        "h_rdocs_1"
      ],
      "metadata": {
        "id": "Czt6whRgurQW"
      },
      "id": "Czt6whRgurQW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code cell,\n",
        "\n",
        "- **h_retriever.get_relevant_documents(...)**: This method queries the retriever (which is linked to the FAISS vector database) with a given text query."
      ],
      "metadata": {
        "id": "kQtPWweLvAcu"
      },
      "id": "kQtPWweLvAcu"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see above, the retriever that was created using FAISS and Hugging Face Embedding is now capable of pulling relavant documents from the original CSV file knowledge store. This is very powerful and it will help us further in this project."
      ],
      "metadata": {
        "id": "0rWlknGZvHwE"
      },
      "id": "0rWlknGZvHwE"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise-10:** Create a FAISS vector database using Embeddings model 'sentence-transformers/all-MiniLM-L6-v2'. Then retrieve relevant answers for a query. Use 'get_relevant_documents()'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "tixqezfDZ3hj"
      },
      "id": "tixqezfDZ3hj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS instance for vector database from 'data'\n",
        "h_vectordb_2 = FAISS.from_documents(documents=data,\n",
        "                                 embedding=embed_model_2)\n",
        "\n",
        "# Create a retriever for querying the vector database\n",
        "h_retriever_2 = h_vectordb_2.as_retriever(score_threshold = 0.7)"
      ],
      "metadata": {
        "id": "aFB0HAh9Y4Pu"
      },
      "id": "aFB0HAh9Y4Pu",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_rdocs_2 = h_retriever_2.get_relevant_documents(\"how about job placement support?\")\n",
        "h_rdocs_2"
      ],
      "metadata": {
        "id": "BwO6HE5gY-cw"
      },
      "id": "BwO6HE5gY-cw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise-11:** Create a FAISS vector database using Embeddings model 'sentence-transformers/paraphrase-MiniLM-L12-v2'. Then retrieve relevant answers for a query. Use 'get_relevant_documents()'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "89X3iUpIaUSU"
      },
      "id": "89X3iUpIaUSU"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS instance for vector database from 'data'\n",
        "h_vectordb_3 = FAISS.from_documents(documents=data,\n",
        "                                 embedding=embed_model_3)\n",
        "\n",
        "# Create a retriever for querying the vector database\n",
        "h_retriever_3 = h_vectordb_3.as_retriever(score_threshold = 0.7)"
      ],
      "metadata": {
        "id": "X1w_fbkCakGG"
      },
      "id": "X1w_fbkCakGG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_rdocs_3 = h_retriever_3.get_relevant_documents(\"how about job placement support?\")\n",
        "h_rdocs_3"
      ],
      "metadata": {
        "id": "nOdlYZQTaw7-"
      },
      "id": "nOdlYZQTaw7-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise-12:** Create a FAISS vector database using Embeddings model 'sentence-transformers/all-distilroberta-v1'. Then retrieve relevant answers for a query. Use 'get_relevant_documents()'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "2ScvX_Dza9Iu"
      },
      "id": "2ScvX_Dza9Iu"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS instance for vector database from 'data'\n",
        "h_vectordb_4 = FAISS.from_documents(documents=data,\n",
        "                                 embedding=embed_model_4)\n",
        "\n",
        "# Create a retriever for querying the vector database\n",
        "h_retriever_4 = h_vectordb_4.as_retriever(score_threshold = 0.7)"
      ],
      "metadata": {
        "id": "q-CZ55rjbKGk"
      },
      "id": "q-CZ55rjbKGk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_rdocs_4 = h_retriever_4.get_relevant_documents(\"how about job placement support?\")\n",
        "h_rdocs_4"
      ],
      "metadata": {
        "id": "CJf10AnLbfMn"
      },
      "id": "CJf10AnLbfMn",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Exercise-13:** Create a FAISS vector database using Embeddings model 'sentence-transformers/multi-qa-MiniLM-L6-cos-v1'. Then retrieve relevant answers for a query. Use 'get_relevant_documents()'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "ZORYEVYsbmfr"
      },
      "id": "ZORYEVYsbmfr"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a FAISS instance for vector database from 'data'\n",
        "h_vectordb_5 = FAISS.from_documents(documents=data,\n",
        "                                 embedding=embed_model_5)\n",
        "\n",
        "# Create a retriever for querying the vector database\n",
        "h_retriever_5 = h_vectordb_5.as_retriever(score_threshold = 0.7)"
      ],
      "metadata": {
        "id": "HAuiGfAFbvrk"
      },
      "id": "HAuiGfAFbvrk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_rdocs_5 = h_retriever_5.get_relevant_documents(\"how about job placement support?\")\n",
        "h_rdocs_5"
      ],
      "metadata": {
        "id": "Sb1WgfD9b6Xh"
      },
      "id": "Sb1WgfD9b6Xh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.4 Create RetrievalQA chain with FAISS Vectore Store & Hugging Face ðŸš€\n",
        "\n",
        "**Exercise-14:** Create RetrievalQA chains for 5 different HuggungFace Embedding Models. Use llm model zephyr_7b_beta and use PromptTemplate to get PROMPT. Then use 'RetrievalQA.from_chain_type()' for getting the 5 Hugging Face RetrievalQA chains. **(0.5 point)**"
      ],
      "metadata": {
        "id": "hmgfg_bnwYQZ"
      },
      "id": "hmgfg_bnwYQZ"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
        "In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
        "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
        "\n",
        "CONTEXT: {context}\n",
        "\n",
        "QUESTION: {question}\"\"\"\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(input_variables=[\"context\", \"question\"], template=prompt_template)\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "h_chain_1 = RetrievalQA.from_chain_type(llm=zephyr_7b_beta_HFE_llm,\n",
        "                            chain_type=\"stuff\",\n",
        "                            retriever=h_retriever_1,\n",
        "                            input_key=\"query\",\n",
        "                            return_source_documents=True,\n",
        "                            chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "h_chain_1\n",
        "\n",
        "h_chain_2 = RetrievalQA.from_chain_type(llm=zephyr_7b_beta_HFE_llm,\n",
        "                            chain_type=\"stuff\",\n",
        "                            retriever=h_retriever_2,\n",
        "                            input_key=\"query\",\n",
        "                            return_source_documents=True,\n",
        "                            chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "h_chain_2\n",
        "\n",
        "h_chain_3 = RetrievalQA.from_chain_type(llm=zephyr_7b_beta_HFE_llm,\n",
        "                            chain_type=\"stuff\",\n",
        "                            retriever=h_retriever_3,\n",
        "                            input_key=\"query\",\n",
        "                            return_source_documents=True,\n",
        "                            chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "h_chain_3\n",
        "\n",
        "h_chain_4 = RetrievalQA.from_chain_type(llm=zephyr_7b_beta_HFE_llm,\n",
        "                            chain_type=\"stuff\",\n",
        "                            retriever=h_retriever_4,\n",
        "                            input_key=\"query\",\n",
        "                            return_source_documents=True,\n",
        "                            chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "h_chain_4\n",
        "\n",
        "h_chain_5 = RetrievalQA.from_chain_type(llm=zephyr_7b_beta_HFE_llm,\n",
        "                            chain_type=\"stuff\",\n",
        "                            retriever=h_retriever_5,\n",
        "                            input_key=\"query\",\n",
        "                            return_source_documents=True,\n",
        "                            chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "h_chain_5"
      ],
      "metadata": {
        "id": "LGmxitlrwhm3"
      },
      "id": "LGmxitlrwhm3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code cell,  The code snippet sets up a RetrievalQA chain using a custom prompt template with a Hugging Face language model and a retriever.\n",
        "\n",
        "- **PromptTemplate(...)**: Initializes a PromptTemplate object from the langchain.prompts module.\n",
        "- **template=prompt_template**: Specifies the template string created above.\n",
        "- **input_variables=[\"context\", \"question\"]**: Defines the placeholders in the template that will be replaced by actual context and question values during the query.\n",
        "- **chain_type_kwargs**: This dictionary contains the prompt key with the PROMPT object, which will be used to format the queries sent to the language model.\n",
        "- **RetrievalQA.from_chain_type(...)**: Initializes a RetrievalQA chain.\n",
        "- **llm=h_llm**: Specifies the language model (h_llm) to be used for generating answers.\n",
        "- **chain_type=\"stuff\"**: Defines the type of chain. In this case, \"stuff\" is a placeholder that can be replaced with other chain types depending on the use case.\n",
        "- **retriever=h_retriever**: Provides the retriever (h_retriever) that will be used to fetch relevant context from the vector database.\n",
        "- **input_key=\"query\"**: Indicates the key used to pass the query to the chain.\n",
        "return_source_documents=True: Ensures that the source documents used to generate the answer are returned along with the answer.\n",
        "- **chain_type_kwargs=chain_type_kwargs**: Passes additional keyword arguments (including the prompt template) to the chain."
      ],
      "metadata": {
        "id": "iqj8LeDGxYlx"
      },
      "id": "iqj8LeDGxYlx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.5 Let's ask some questions to FAISS based Hugging Face RetrievalQA chain\n",
        "\n",
        "**Exercise-15:** Execute a retrieval-based QA query for the question: 'Do you provide job assistance and also do you provide job guarantee?' using each of the 5 ReyrievalQA chains as achieved in Exercise-14. **(0.5 point)**"
      ],
      "metadata": {
        "id": "hUfOxRlZxfSY"
      },
      "id": "hUfOxRlZxfSY"
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = 'Do you provide job assistance and also do you provide job gurantee?'\n",
        "\n",
        "h_retrieval_QA1 = h_chain_1.invoke(Q1)\n",
        "h_retrieval_QA1\n",
        "\n",
        "# Get the list of keys in the dictionary\n",
        "keys_list = list(h_retrieval_QA1.keys())\n",
        "\n",
        "# Access the value using the key's index\n",
        "h_result_value1 = h_retrieval_QA1[keys_list[1]]  # 1 is the index of 'result' key\n",
        "#print(h_result_value1)\n",
        "h_result_value1\n",
        "######################################################\n",
        "h_retrieval_QA2 = h_chain_2.invoke(Q1)\n",
        "h_retrieval_QA2\n",
        "\n",
        "# Get the list of keys in the dictionary\n",
        "keys_list = list(h_retrieval_QA2.keys())\n",
        "\n",
        "# Access the value using the key's index\n",
        "h_result_value2 = h_retrieval_QA2[keys_list[1]]  # 1 is the index of 'result' key\n",
        "#print(h_result_value2)\n",
        "h_result_value2\n",
        "######################################################\n",
        "h_retrieval_QA3 = h_chain_3.invoke(Q1)\n",
        "h_retrieval_QA3\n",
        "\n",
        "# Get the list of keys in the dictionary\n",
        "keys_list = list(h_retrieval_QA3.keys())\n",
        "\n",
        "# Access the value using the key's index\n",
        "h_result_value3 = h_retrieval_QA3[keys_list[1]]  # 1 is the index of 'result' key\n",
        "#print(h_result_value3)\n",
        "h_result_value3\n",
        "######################################################\n",
        "h_retrieval_QA4 = h_chain_4.invoke(Q1)\n",
        "h_retrieval_QA4\n",
        "\n",
        "# Get the list of keys in the dictionary\n",
        "keys_list = list(h_retrieval_QA4.keys())\n",
        "\n",
        "# Access the value using the key's index\n",
        "h_result_value4 = h_retrieval_QA4[keys_list[1]]  # 1 is the index of 'result' key\n",
        "#print(h_result_value4)\n",
        "h_result_value4\n",
        "######################################################\n",
        "h_retrieval_QA5 = h_chain_5.invoke(Q1)\n",
        "h_retrieval_QA5\n",
        "\n",
        "# Get the list of keys in the dictionary\n",
        "keys_list = list(h_retrieval_QA5.keys())\n",
        "\n",
        "# Access the value using the key's index\n",
        "h_result_value5 = h_retrieval_QA5[keys_list[1]]  # 1 is the index of 'result' key\n",
        "#print(h_result_value5)\n",
        "h_result_value5"
      ],
      "metadata": {
        "id": "RHH8wCm8xleh"
      },
      "id": "RHH8wCm8xleh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**As you can see above, the answer of question comes from two different FAQs within the Codebasics FAQ csv file and it is able to pull those questions and merge them nicely.**"
      ],
      "metadata": {
        "id": "LOI1SQakxr3_"
      },
      "id": "LOI1SQakxr3_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.6 Comparison: 5 different embedding models performance (for FAISS Vectore Store)\n",
        "\n",
        "**Exercise-16:** Compare the RetrievalQA performance among all 5 different Embedding Models using Cosine Similarity.\n",
        "\n",
        "Use the embeddig models achieved under section 3.2. **(0.5 point)**\n",
        "\n",
        "- **(a)** Consider the reference Question: 'Do you provide job assistance and also do you provide job guarantee?'. Compute Cosine Similarity.\n",
        "\n",
        "- **(b)** Consider the Benchmark_response: 'Yes, We help you with resume and interview preparation along with that we help you in building online credibility, and based on requirements we refer candidates to potential recruiters.' Compute Cosine Similarity."
      ],
      "metadata": {
        "id": "FVuehG4whb_g"
      },
      "id": "FVuehG4whb_g"
    },
    {
      "cell_type": "code",
      "source": [
        "h_embeddings1 = embed_model_1\n",
        "h_embeddings2 = embed_model_2\n",
        "h_embeddings3 = embed_model_3\n",
        "h_embeddings4 = embed_model_4\n",
        "h_embeddings5 = embed_model_5\n",
        "\n",
        "Benchmark_response = \"\"\"Yes, We help you with resume and interview preparation along with that we help you in building online credibility,\n",
        "and based on requirements we refer candidates to potential recruiters.\"\"\"\n",
        "\n",
        "BMR = Benchmark_response\n",
        "\n",
        "Q1_h_e1 = np.array(h_embeddings1.embed_query(Q1)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "BMR_h_e1 = np.array(h_embeddings1.embed_query(BMR)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "h_e1 = np.array(h_embeddings1.embed_query(h_result_value1)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "\n",
        "Q1_h_e2 = np.array(h_embeddings2.embed_query(Q1)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "BMR_h_e2 = np.array(h_embeddings2.embed_query(BMR)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "h_e2 = np.array(h_embeddings2.embed_query(h_result_value2)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "\n",
        "Q1_h_e3 = np.array(h_embeddings3.embed_query(Q1)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "BMR_h_e3 = np.array(h_embeddings3.embed_query(BMR)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "h_e3 = np.array(h_embeddings3.embed_query(h_result_value3)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "\n",
        "Q1_h_e4 = np.array(h_embeddings4.embed_query(Q1)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "BMR_h_e4 = np.array(h_embeddings4.embed_query(BMR)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "h_e4 = np.array(h_embeddings4.embed_query(h_result_value4)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "\n",
        "Q1_h_e5 = np.array(h_embeddings5.embed_query(Q1)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "BMR_h_e5 = np.array(h_embeddings5.embed_query(BMR)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "h_e5 = np.array(h_embeddings5.embed_query(h_result_value5)).reshape(1, -1)  # Convert to array and reshape to 2D"
      ],
      "metadata": {
        "id": "nZNw9BCNjLGM"
      },
      "id": "nZNw9BCNjLGM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# (a)\n",
        "# Compute cosine similarity\n",
        "cosine_sim_1 = cosine_similarity(Q1_h_e1, h_e1)[0][0]\n",
        "cosine_sim_2 = cosine_similarity(Q1_h_e2, h_e2)[0][0]\n",
        "cosine_sim_3 = cosine_similarity(Q1_h_e3, h_e3)[0][0]\n",
        "cosine_sim_4 = cosine_similarity(Q1_h_e4, h_e4)[0][0]\n",
        "cosine_sim_5 = cosine_similarity(Q1_h_e5, h_e5)[0][0]\n",
        "\n",
        "print(f\"Cosine Similarity between Q1 and h_result_value1: {cosine_sim_1}\")\n",
        "print(f\"Cosine Similarity between Q1 and h_result_value2: {cosine_sim_2}\")\n",
        "print(f\"Cosine Similarity between Q1 and h_result_value3: {cosine_sim_3}\")\n",
        "print(f\"Cosine Similarity between Q1 and h_result_value4: {cosine_sim_4}\")\n",
        "print(f\"Cosine Similarity between Q1 and h_result_value5: {cosine_sim_5}\")"
      ],
      "metadata": {
        "id": "5JZxzzezmQaU"
      },
      "id": "5JZxzzezmQaU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So, by considering the reference query Q1, as we can observe from the above result, that the highest Cosine Similarity (75.068%) is achieved by using the HuggingFace embedding model 'BAAI/bge-small-en-v1.5'. So after the below code cell, we will use the corresponding RetrievalQA chain (i.e., h_chain_1 which is the best out of 5 RetrievalQA chains) to ask following queries and to get responses.**"
      ],
      "metadata": {
        "id": "QWGD__2rnYPz"
      },
      "id": "QWGD__2rnYPz"
    },
    {
      "cell_type": "code",
      "source": [
        "# (b)\n",
        "# Compute cosine similarity\n",
        "cosine_sim_1 = cosine_similarity(BMR_h_e1, h_e1)[0][0]\n",
        "cosine_sim_2 = cosine_similarity(BMR_h_e2, h_e2)[0][0]\n",
        "cosine_sim_3 = cosine_similarity(BMR_h_e3, h_e3)[0][0]\n",
        "cosine_sim_4 = cosine_similarity(BMR_h_e4, h_e4)[0][0]\n",
        "cosine_sim_5 = cosine_similarity(BMR_h_e5, h_e5)[0][0]\n",
        "\n",
        "print(f\"Cosine Similarity between BMR and h_result_value1: {cosine_sim_1}\")\n",
        "print(f\"Cosine Similarity between BMR and h_result_value2: {cosine_sim_2}\")\n",
        "print(f\"Cosine Similarity between BMR and h_result_value3: {cosine_sim_3}\")\n",
        "print(f\"Cosine Similarity between BMR and h_result_value4: {cosine_sim_4}\")\n",
        "print(f\"Cosine Similarity between BMR and h_result_value5: {cosine_sim_5}\")"
      ],
      "metadata": {
        "id": "ejEkBm9u3vc_"
      },
      "id": "ejEkBm9u3vc_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**So, by considering the Benchmark_response BMR, as we can observe from the above result, that the highest Cosine Similarity (81.883%) is achieved by using the embedding model 'sentence-transformers/all-distilroberta-v1'.**"
      ],
      "metadata": {
        "id": "doWpjEDh4OfM"
      },
      "id": "doWpjEDh4OfM"
    },
    {
      "cell_type": "code",
      "source": [
        "h_chain_1(\"Do you guys provide internship and also do you offer EMI payments?\")"
      ],
      "metadata": {
        "id": "G2WqFoJrxyzY"
      },
      "id": "G2WqFoJrxyzY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_chain_1(\"do you have javascript course?\")"
      ],
      "metadata": {
        "id": "Z-63uWBdx1T_"
      },
      "id": "Z-63uWBdx1T_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_chain_1(\"Do you have plans to launch blockchain course in future?\")"
      ],
      "metadata": {
        "id": "S0VJ48Olx4Co"
      },
      "id": "S0VJ48Olx4Co",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_chain_1(\"should I learn power bi or tableau?\")"
      ],
      "metadata": {
        "id": "KryCvsdhx6mb"
      },
      "id": "KryCvsdhx6mb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_chain_1(\"I've a MAC computer. Can I use powerbi on it?\")"
      ],
      "metadata": {
        "id": "MP_ZrpkHx9LY"
      },
      "id": "MP_ZrpkHx9LY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_chain_1(\"I don't see power pivot. how can I enable it?\")"
      ],
      "metadata": {
        "id": "t3JvaRNTx_tH"
      },
      "id": "t3JvaRNTx_tH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "h_chain_1(\"What is the price of your machine learning course?\")"
      ],
      "metadata": {
        "id": "774_F3gRyCKQ"
      },
      "id": "774_F3gRyCKQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Phase-IV:** Performing RAG using HuggingFace Retrieval Chain For Fixed Embedding model and Chromadb Vector Store\n",
        "\n",
        "In this Phase-IV, the vector store is changed from FAISS to Chromadb\n",
        "\n",
        "<br><br>\n",
        "<center>\n",
        "<img src=\" https://cdn.exec.talentsprint.com/static/cds/content/varying_vector_stores-5.png\" height = 600 width= 1600 px/>\n",
        "</center>\n",
        "<br><br>"
      ],
      "metadata": {
        "id": "3r6qc23JnHZe"
      },
      "id": "3r6qc23JnHZe"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.1 Vector store using Chromadb\n",
        "\n",
        "##### For vector database we can use chromadb as shown below. During the experimentation, we found Hugging Face Embeddings and FAISS to be appropriate for our use case. Let's see the retrieval performance using Chromadb in the following code cell.\n",
        "\n",
        "**Exercise-17:** Create a Chroma vector database. Use the above achieved best Hugging Face Embeddings model 'BAAI/bge-small-en-v1.5'. Then retrieve relevant answers for a query. Use 'get_relevant_documents()' **(0.5 point)**"
      ],
      "metadata": {
        "id": "I5NU2wZhqpWu"
      },
      "id": "I5NU2wZhqpWu"
    },
    {
      "cell_type": "code",
      "source": [
        "g_vectordb_1 = Chroma.from_documents(data, embedding=embed_model_1, persist_directory='./chromadb')\n",
        "g_vectordb_1.persist()"
      ],
      "metadata": {
        "id": "vf69UpUXrHIP"
      },
      "id": "vf69UpUXrHIP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a retriever for querying the vector database derived through Chroma\n",
        "g_retriever_1 = g_vectordb_1.as_retriever(score_threshold = 0.7)"
      ],
      "metadata": {
        "id": "ipLGSB14rOjj"
      },
      "id": "ipLGSB14rOjj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_rdocs_1 = g_retriever_1.get_relevant_documents(\"how about job placement support?\")\n",
        "g_rdocs_1"
      ],
      "metadata": {
        "id": "co-yOQsSrWQK"
      },
      "id": "co-yOQsSrWQK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code cell,\n",
        "\n",
        "- **Chroma.from_documents(...)**: This method initializes a Chroma vector database using a list of documents, an embedding model, and a directory to persist the database.\n",
        "- **g_vectordb.as_retriever(...)**: This method converts the Chroma vector database instance (g_vectordb) into a retriever object that can be used to perform queries.\n",
        "- **g_retriever.get_relevant_documents(...)**: This method queries the retriever object (g_retriever) with the given text query."
      ],
      "metadata": {
        "id": "Ka1skkNwrg42"
      },
      "id": "Ka1skkNwrg42"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.2 Create RetrievalQA chain with Chromadb Vectore Store & Hugging Face ðŸš€\n",
        "\n",
        "**Exercise-18:** Now we will use the achieved best embedding model as evaluated in Exercise-16 (i.e., HuggingFace embedding model 'BAAI/bge-small-en-v1.5') to see if there is any impact in RetrievalQA chain's performance if the Vector Store is changed from FAISS to Chromadb. Create RetrievalQA chain with Chromadb Vectore Store. Use PromptTemplate to get PROMPT. Then use 'RetrievalQA.from_chain_type()' for getting the Chromadb Vectore Store based RetrievalQA chain. **(0.5 point)**"
      ],
      "metadata": {
        "id": "Qw2kodBOJSWp"
      },
      "id": "Qw2kodBOJSWp"
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = \"\"\"Given the following context and a question, generate an answer based on this context only.\n",
        "In the answer try to provide as much text as possible from \"response\" section in the source document context without making much changes.\n",
        "If the answer is not found in the context, kindly state \"I don't know.\" Don't try to make up an answer.\n",
        "\n",
        "CONTEXT: {context}\n",
        "\n",
        "QUESTION: {question}\"\"\"\n",
        "\n",
        "\n",
        "PROMPT = PromptTemplate(\n",
        "    template=prompt_template, input_variables=[\"context\", \"question\"]\n",
        ")\n",
        "chain_type_kwargs = {\"prompt\": PROMPT}\n",
        "\n",
        "g_chain = RetrievalQA.from_chain_type(llm=zephyr_7b_beta_HFE_llm,\n",
        "                            chain_type=\"stuff\",\n",
        "                            retriever=g_retriever_1,\n",
        "                            input_key=\"query\",\n",
        "                            return_source_documents=True,\n",
        "                            chain_type_kwargs=chain_type_kwargs)\n",
        "\n",
        "g_chain"
      ],
      "metadata": {
        "id": "CFPOVQL-KHDa"
      },
      "id": "CFPOVQL-KHDa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code cell, The code snippet sets up a RetrievalQA chain using a custom prompt template with a Google PaLM language model and a retriever.\n",
        "\n",
        "- **PromptTemplate(...)**: Initializes a PromptTemplate object from the langchain.prompts module.\n",
        "- **template=prompt_template**: Specifies the template string that defines how queries should be formatted.\n",
        "- **input_variables=[\"context\", \"question\"]**: Lists the placeholders in the template that will be replaced by actual values for context and question.\n",
        "chain_type_kwargs: A dictionary that includes the prompt template used to format the queries.\n",
        "- **RetrievalQA.from_chain_type(...)**: Initializes a RetrievalQA chain.\n",
        "- **llm=g_llm**: Specifies the Google PaLM language model (g_llm) used for generating answers.\n",
        "- **chain_type=\"stuff\"**: Defines the type of chain. \"stuff\" can be replaced with other chain types as needed.\n",
        "- **retriever=g_retriever**: The retriever (g_retriever) used to fetch relevant documents from the vector database.\n",
        "- **input_key=\"query\"**: Indicates the key used for passing the query to the chain.\n",
        "- **return_source_documents=True**: Ensures that the documents used to generate the answer are returned along with the answer.\n",
        "- **chain_type_kwargs=chain_type_kwargs**: Passes additional keyword arguments, including the prompt template, to the chain."
      ],
      "metadata": {
        "id": "uu-CsmY_KiLm"
      },
      "id": "uu-CsmY_KiLm"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.3 Let's ask some questions to Chromadb based HuggingFace retrieval QA chain\n",
        "\n",
        "**Exercise-19:** By using the Chromadb Vector Store based Retrieval QA chain (achieved in Exercise-18), execute a retrieval-based QA query for the question: 'Do you provide job assistance and also do you provide job guarantee?'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "Z1SfPyW3KqFO"
      },
      "id": "Z1SfPyW3KqFO"
    },
    {
      "cell_type": "code",
      "source": [
        "Q1 = 'Do you provide job assistance and also do you provide job gurantee?'\n",
        "g_retrieval_QA1 = g_chain(Q1)\n",
        "g_retrieval_QA1\n",
        "\n",
        "# Get the list of keys in the dictionary\n",
        "keys_list = list(g_retrieval_QA1.keys())\n",
        "\n",
        "# Access the value using the key's index\n",
        "g_result_value1 = g_retrieval_QA1[keys_list[1]]  # 1 is the index of 'result' key\n",
        "#print(g_result_value1)\n",
        "g_result_value1"
      ],
      "metadata": {
        "id": "iwIB6EVpLb8Z"
      },
      "id": "iwIB6EVpLb8Z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4.4 Comparison: Is there any impact?\n",
        "- keeping the llm and embedding model unchanged but only changing the Vector Store from FAISS to Chromadb\n",
        "\n",
        "**Exercise-20:** Using Cosine Similarity, measure the RetrievalQA performance of the Chromadb based RetrievalQA chain as achieved in Exercise-18. Use the best embeddig model as evaluated in Exercise-16 (i.e., HuggingFace embedding model 'BAAI/bge-small-en-v1.5').\n",
        "\n",
        "Consider the reference Question: 'Do you provide job assistance and also do you provide job guarantee?'. **(0.5 point)**"
      ],
      "metadata": {
        "id": "KaxvDh6qs_oL"
      },
      "id": "KaxvDh6qs_oL"
    },
    {
      "cell_type": "code",
      "source": [
        "# Using HuggingFaceEmbeddings 'BAAI/bge-small-en-v1.5'\n",
        "embeddings = embed_model_1\n",
        "Q1_e = np.array(embeddings.embed_query(Q1)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "g_e1 = np.array(embeddings.embed_query(g_result_value1)).reshape(1, -1)  # Convert to array and reshape to 2D\n",
        "\n",
        "#g_e1 = np.array(embeddings.embed_query(g_result_value1)).reshape(1, -1)  # Convert to array and reshape to 2D"
      ],
      "metadata": {
        "id": "LZ91qia2u2c9"
      },
      "id": "LZ91qia2u2c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compute cosine similarity\n",
        "cosine_sim_Chromadb = cosine_similarity(Q1_e, g_e1)[0][0]\n",
        "#cosine_sim_Chromadb = cosine_similarity(Q1_e, g_e1)[0][0]\n",
        "\n",
        "#print(f\"Cosine Similarity between Q1 and FAISS based h_result_value1: {cosine_sim_FAISS}\")\n",
        "print(f\"Cosine Similarity between Q1 and Chromadb based g_result_value1: {cosine_sim_Chromadb}\")"
      ],
      "metadata": {
        "id": "ZxPG-x8jvPyc"
      },
      "id": "ZxPG-x8jvPyc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Cosine Similarity between Q1 and h_result_value1: {cosine_sim_1}\")\n",
        "print(f\"Difference in Cosine Similarity between FAISS and Chromadb: {cosine_sim_1 - cosine_sim_Chromadb}\")\n",
        "print(f\"Percentage Difference in Cosine Similarity between FAISS and Chromadb: {(cosine_sim_1 - cosine_sim_Chromadb)*100}%\")"
      ],
      "metadata": {
        "id": "eOivgmHIwQw9"
      },
      "id": "eOivgmHIwQw9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Hence, from the above result we can observe that in RAG performance, there is 0.0665% difference (i.e., very low difference) in Cosine Similarity between FAISS and Chromadb based retrieval chain if the llm and embedding model are remained unchanged. So, there is very less impact of changing the Vector Store, if the llm and embedinng model remain same.**"
      ],
      "metadata": {
        "id": "12nw5P-5w-dv"
      },
      "id": "12nw5P-5w-dv"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Optional Task:** Execute the below code cells to test the RAG performance with the following queries. Use Chromadb based RetrievalQA chain as obtained in Exercise-18."
      ],
      "metadata": {
        "id": "UtvA_G-VyaND"
      },
      "id": "UtvA_G-VyaND"
    },
    {
      "cell_type": "code",
      "source": [
        "g_chain(\"do you have javascript course?\")"
      ],
      "metadata": {
        "id": "TJE7fBxfLjdR"
      },
      "id": "TJE7fBxfLjdR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_chain(\"Do you have plans to launch blockchain course in future?\")"
      ],
      "metadata": {
        "id": "z36Slr3HLnhP"
      },
      "id": "z36Slr3HLnhP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_chain(\"should I learn power bi or tableau?\")"
      ],
      "metadata": {
        "id": "5tmPCGkbLqzx"
      },
      "id": "5tmPCGkbLqzx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_chain(\"I've a MAC computer. Can I use powerbi on it?\")"
      ],
      "metadata": {
        "id": "IIDDyInyLuDR"
      },
      "id": "IIDDyInyLuDR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_chain(\"I don't see power pivot. how can I enable it?\")"
      ],
      "metadata": {
        "id": "6IR1zM2iLzTi"
      },
      "id": "6IR1zM2iLzTi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "g_chain(\"What is the price of your machine learning course?\")"
      ],
      "metadata": {
        "id": "epIJt6kXL2mc"
      },
      "id": "epIJt6kXL2mc",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}